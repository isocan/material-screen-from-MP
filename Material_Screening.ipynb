{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aYyLc7AC0JWLmkJsl4T-VpJ50rRVcNWg",
      "authorship_tag": "ABX9TyO7yzX7sJegLcU8dUYIFzPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isocan/material-screen-from-MP/blob/main/Material_Screening.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"sec-introduction\"></a> Introduction\n"
      ],
      "metadata": {
        "id": "B9WLhQ5_w7WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Material Screening Workflow\n",
        "\n",
        "This notebook demonstrates a systematic approach to materials screening using the Materials Project (MP) database. It specifically focuses on identifying bimetallic compounds, categorizing them as \"in-domain\" or \"out-of-domain\" based on their presence in the Open Catalyst 2020 (OC20) dataset, and enriching the data with additional computed properties from the Materials Project."
      ],
      "metadata": {
        "id": "Q-kTf7M8GWPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents  \n",
        "\n",
        "* **[1. Introduction](#sec-introduction)**\n",
        "* **[2. Prerequisites & Environment](#sec-env)**\n",
        "* **[3. Data Sources](#sec-data-sources)**\n",
        "* **[4. Step 1 – Query bimetallic entries](#sec-step1-query)**\n",
        "* **[5. Step 2 – Label “in‑domain” vs “out‑of‑domain”](#sec-step2-label)**\n",
        "* **[6. Step 3 – Enrich missing symmetry & properties](#sec-step3-enrich)**\n",
        "  * [6.1 Predict symmetry locally](#subsec-symmetry-pred)\n",
        "  * [6.2 Fill numeric gaps by composition match](#subsec-gap-fill)\n",
        "* **[7. Step 4 – Exploratory visualisation](#sec-step4-viz)**\n",
        "* **[8. Step 5 – Generate VASP input decks](#sec-step5-vasp)**\n",
        "* **[9. Step 6 – Build the Out‑of‑Domain Bulk Pickle from VASP OUTCARs](#sec-outcar-extract)**\n",
        "* **[10. Step 7 – Package merged pickle](#sec-step7-package)**\n",
        "* **[11. Conclusions](#sec-conclusions)**\n",
        "* **[12. References](#sec-references)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WqUAXlrsGc9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-env\"></a> Prerequisites & Environment\n"
      ],
      "metadata": {
        "id": "QonvKTW8QRiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Verify Python environment"
      ],
      "metadata": {
        "id": "SgrRrxCbRG3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "_VK2DInpQQgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install and upgrade dependencies\n",
        "\n"
      ],
      "metadata": {
        "id": "NIbp-ksURdER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install pandas mp-api pymatgen ase tqdm matplotlib"
      ],
      "metadata": {
        "id": "Ow-HShy0Rban"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Import core libraries"
      ],
      "metadata": {
        "id": "_6x2h7UDRo3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Standard library\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from pathlib import Path              # file paths / saving results\n",
        "from itertools import combinations    # generate element pairs\n",
        "import pickle                         # serialize filtered bulks (in_domain.pkl)\n",
        "import ast                            # safe literal parsing (if needed for lists/tuples)\n",
        "import os\n",
        "import random\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Third‑party scientific stack\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import numpy as np                    # numeric utilities\n",
        "import pandas as pd                   # tabular data wrangling\n",
        "from tqdm import tqdm                 # progress bars for API loops\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import NullLocator\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Atomistic / materials toolkits\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from ase import Atoms                                 # ASE structures in bulks.pkl\n",
        "from ase.io import read\n",
        "from ase.io import write\n",
        "from ase.calculators.singlepoint import PropertyNotImplementedError\n",
        "from ase.calculators.singlepoint import SinglePointCalculator\n",
        "from mp_api.client import MPRester                    # Materials Project REST client\n",
        "from pymatgen.core import Composition                 # chemistry utilities (parsing formulas)\n",
        "from pymatgen.io.ase import AseAtomsAdaptor           # ASE↔pymatgen structure conversion\n",
        "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer  # space‑group detection\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ahMsD1KPX9Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ⚠️ **Materials Project API key required**  \n",
        "> Set it once per shell session  \n",
        "> `export MP_API_KEY=\"YOUR_KEY\"`  \n",
        "> **or** replace the placeholder below:  \n",
        "> ```python\n",
        "> # Replace with your Materials Project API key\n",
        "> API_KEY = \"XXXXXXXXX\"\n",
        "> ```\n"
      ],
      "metadata": {
        "id": "KOmJyOy-Mb94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-data-sources\"></a> Data Sources\n"
      ],
      "metadata": {
        "id": "h5xB6j7mx5mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⬇️ Loading the OC20 *bulks.pkl* dataset  \n",
        "\n",
        "The **Open Catalyst Project** distributes pre‑computed bulk crystal structures\n",
        "in a single pickle file, **`bulks.pkl`**.  \n",
        "Each element of the list stored in this file is a small Python dictionary with:\n",
        "\n",
        "| key | description |\n",
        "|-----|-------------|\n",
        "| `atoms` | an **ASE `Atoms`** object containing lattice vectors, atomic coordinates, and—if available—a `SinglePointCalculator` with the final DFT results |\n",
        "| `src_id` | the canonical Materials Project identifier (e.g. `mp‑1234`) |\n",
        "| `bulk_sampling_str` | OC20’s internal bookkeeping string for sampling provenance |\n",
        "\n",
        "You can download the file directly from the project repository:  \n",
        "<https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/databases/pkls/bulks.pkl>"
      ],
      "metadata": {
        "id": "NdwtRjGk1AvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Load OC20 bulks ────────────────────────────────────────────────────────\n",
        "with open(\"/content/bulks.pkl\", \"rb\") as f:\n",
        "    bulks = pickle.load(f)   # → list[dict]\n",
        "\n",
        "print(f\"Loaded {len(bulks):,} bulk structures from OC20.\")\n",
        "\n",
        "# Quick sanity‑checks\n",
        "first = bulks[0]\n",
        "print(\"Keys in first entry:\", list(first.keys()))\n",
        "print(\"Example src_id:\", first['src_id'])\n",
        "print(\"ASE Atoms object:\", first['atoms'])"
      ],
      "metadata": {
        "id": "CWgjQs00xzXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Bimetallic‐pair statistics  \n",
        "The cell below scans the **`bulks`** list (loaded earlier) and counts how many\n",
        "times every unique two‑metal combination appears.  \n",
        "We then\n",
        "\n",
        "1.  list the **most common** and **least common** pairs, and  \n",
        "2.  draw a quick heat‑map so you can spot dominant vs. sparse regions at a glance."
      ],
      "metadata": {
        "id": "xgUJ-4_F7dgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Parameters\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "symbols_of_interest = {\n",
        "    \"Ir\",\"Pt\",\"Ru\",\"Ni\",\"Fe\",\"Mo\",\"Ta\",\"Nb\",\"Ti\",\"Cu\",\"Sb\",\"Mn\",\"Co\",\"Sn\"\n",
        "}\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Build a DataFrame of all bimetal pairs in `bulks`\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "rows = []\n",
        "for entry in bulks:                           # `bulks` was loaded from bulks.pkl\n",
        "    atoms = entry[\"atoms\"]                   # ASE Atoms\n",
        "    elems = sorted(set(atoms.get_chemical_symbols()))\n",
        "    # keep only pairs fully within our element list\n",
        "    if len(elems) == 2 and set(elems).issubset(symbols_of_interest):\n",
        "        rows.append({\"Ametal\": elems[0], \"Bmetal\": elems[1]})\n",
        "\n",
        "df_pairs = pd.DataFrame(rows)\n",
        "\n",
        "# count frequencies\n",
        "pair_counts = (\n",
        "    df_pairs\n",
        "    .groupby([\"Ametal\", \"Bmetal\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        "    .sort_values(\"count\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# ─── 1) Text summary ────────────────────────────────────────────────────────\n",
        "TOP_N = 10\n",
        "print(f\"\\n🔝 Top {TOP_N} most frequent bimetallic pairs\")\n",
        "display(pair_counts.head(TOP_N))\n",
        "\n",
        "print(f\"\\n🔻 Bottom {TOP_N} least frequent pairs\")\n",
        "display(pair_counts.tail(TOP_N))\n",
        "\n",
        "# ─── 2) Heat‑map visualisation ──────────────────────────────────────────────\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pivot = (\n",
        "    pair_counts\n",
        "    .pivot(index=\"Ametal\", columns=\"Bmetal\", values=\"count\")\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(pivot.values, cmap=\"viridis\")\n",
        "\n",
        "# axis labels and ticks\n",
        "ax.set_xticks(range(len(pivot.columns)))\n",
        "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
        "ax.set_yticks(range(len(pivot.index)))\n",
        "ax.set_yticklabels(pivot.index)\n",
        "\n",
        "# colour‑bar\n",
        "cbar = fig.colorbar(im, ax=ax)\n",
        "cbar.set_label(\"Number of occurrences\")\n",
        "\n",
        "ax.set_title(\"Frequency heat‑map of bimetallic pairs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uEjiQqak7IWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step1-query\"></a> Step 1 – Query Bimetallic Entries\n"
      ],
      "metadata": {
        "id": "-PU7nlQTy14V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔄 High‑throughput retrieval & labelling of bimetallic entries  \n",
        "This code block:\n",
        "\n",
        "1. **Enumerates every unique binary combination** of the 14 transition‑metal / post‑metal elements listed in `symbols`.  \n",
        "2. **Queries the Materials Project** for each pair, requesting a compact but information‑rich set of fields (`FIELDS`).  \n",
        "3. **Builds a tidy pandas table** (`df`) with crystal‑system, space‑group, formation‑energy, band‑gap and other key attributes.  \n",
        "4. **Adds an `experimentally_observed` flag** – `True` when the MP record is tagged as *non‑theoretical* (or has an ICSD entry), `False` otherwise.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E01wksoN8MbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your Materials Project API key (or set MP_API_KEY env var)\n",
        "API_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Elements to search for\n",
        "symbols = [\n",
        "    \"Ir\", \"Pt\", \"Ru\", \"Ni\", \"Fe\", \"Mo\",\n",
        "    \"Ta\", \"Nb\", \"Ti\", \"Cu\", \"Sb\", \"Mn\",\n",
        "    \"Co\", \"Sn\"]\n",
        "\n",
        "FIELDS = [\n",
        "    \"material_id\",\n",
        "    \"chemsys\",\n",
        "    \"formula_pretty\",\n",
        "    \"symmetry\",                   # full object → symbol, number, crystal_system, point_group\n",
        "    \"formation_energy_per_atom\",\n",
        "    \"band_gap\",\n",
        "    \"volume\",\n",
        "    \"structure\",                  # for volume fallback\n",
        "    \"elements\",\n",
        "    \"nelements\",\n",
        "    \"density\",\n",
        "    \"energy_above_hull\",\n",
        "]\n",
        "\n",
        "bimetal_docs = []\n",
        "with MPRester(API_KEY) as mpr:\n",
        "    seen = set()\n",
        "    for a in tqdm(symbols, desc=\"outer\"):\n",
        "        seen.add(a)\n",
        "        for b in symbols:\n",
        "            if b in seen:      # skip reversed duplicates\n",
        "                continue\n",
        "            docs = mpr.materials.summary.search(\n",
        "                elements=[a, b],\n",
        "                num_elements=2,\n",
        "                fields=FIELDS,\n",
        "                chunk_size=100,\n",
        "            )\n",
        "            bimetal_docs.extend(docs)\n",
        "\n",
        "print(f\"Fetched {len(bimetal_docs)} bimetallic entries with full fields.\")\n"
      ],
      "metadata": {
        "id": "b0_5PVt3RoQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_CSV = Path(\"bimetal_tidy.csv\")\n",
        "\n",
        "rows = []\n",
        "for doc in bimetal_docs:\n",
        "    sym = getattr(doc, \"symmetry\", None)\n",
        "\n",
        "    space_group     = getattr(sym, \"symbol\", None) if sym else None\n",
        "    sg_number       = getattr(sym, \"number\", None) if sym else None\n",
        "    crystal_system  = getattr(sym, \"crystal_system\", None) if sym else None\n",
        "    point_group     = getattr(sym, \"point_group\", None) if sym else None\n",
        "\n",
        "    vol = getattr(doc, \"volume\", None)\n",
        "    if vol is None and getattr(doc, \"structure\", None):\n",
        "        try:\n",
        "            vol = doc.structure.volume\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"bulk_id\":                    doc.material_id,\n",
        "            \"chemsys\":                    doc.chemsys,\n",
        "            \"formula_pretty\":             doc.formula_pretty,\n",
        "            \"nelements\":                  getattr(doc, \"nelements\", None),\n",
        "            \"elements\":                   getattr(doc, \"elements\", None),\n",
        "            \"band_gap\":                   getattr(doc, \"band_gap\", None),\n",
        "            \"formation_energy_per_atom\":  getattr(doc, \"formation_energy_per_atom\", None),\n",
        "            \"energy_above_hull\":          getattr(doc, \"energy_above_hull\", None),\n",
        "            \"density\":                    getattr(doc, \"density\", None),\n",
        "            \"volume\":                     vol,\n",
        "            \"spacegroup\":                 space_group,\n",
        "            \"sg_number\":                  sg_number,\n",
        "            \"crystal_system\":             crystal_system,\n",
        "            \"point_group\":                point_group,\n",
        "        }\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# stable column order\n",
        "cols = [\n",
        "    \"chemsys\", \"bulk_id\",\"formula_pretty\",\"crystal_system\", \"spacegroup\",\"volume\", \"formation_energy_per_atom\",\n",
        "     \"band_gap\"]\n",
        "\n",
        "df = df[cols].drop_duplicates(\"bulk_id\").reset_index(drop=True)\n",
        "\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"saved → {OUT_CSV.resolve()}\")\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "1hdh1BxOeTEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_experimentally_observed(df: pd.DataFrame, api_key: str, batch=100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add a boolean column 'experimentally_observed' using MP Summary 'theoretical' flag.\n",
        "    Falls back to checking 'icsd_ids' from get_data_by_id when needed.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"experimentally_observed\"] = pd.NA\n",
        "\n",
        "    ids = df[\"bulk_id\"].dropna().astype(str).unique().tolist()\n",
        "    if not ids:\n",
        "        return df\n",
        "\n",
        "    with MPRester(api_key) as mpr:\n",
        "        # ---- primary: use Summary search → theoretical ----\n",
        "        fields = [\"material_id\", \"theoretical\"]\n",
        "        for i in range(0, len(ids), batch):\n",
        "            chunk = ids[i:i+batch]\n",
        "            docs = mpr.materials.summary.search(material_ids=chunk, fields=fields, chunk_size=len(chunk))\n",
        "            for d in docs:\n",
        "                df.loc[df[\"bulk_id\"] == d.material_id, \"experimentally_observed\"] = (not bool(d.theoretical))\n",
        "\n",
        "        # ---- fallback: if anything still NA, check for ICSD entries ----\n",
        "        missing = df[\"experimentally_observed\"].isna()\n",
        "        if missing.any():\n",
        "            for mid in df.loc[missing, \"bulk_id\"].astype(str).unique():\n",
        "                try:\n",
        "                    # use_document_model=False returns plain dict; safer for probing keys\n",
        "                    doc = mpr.materials.summary.get_data_by_id(mid, fields=[\"theoretical\", \"icsd_ids\"],\n",
        "                                                               use_document_model=False)\n",
        "                    # get_data_by_id may return a list or a dict depending on version; normalize\n",
        "                    if isinstance(doc, list) and doc:\n",
        "                        doc = doc[0]\n",
        "                    theoretical = doc.get(\"theoretical\", None)\n",
        "                    icsd_ids = doc.get(\"icsd_ids\", []) or []\n",
        "                    if theoretical is not None:\n",
        "                        val = not bool(theoretical)\n",
        "                    else:\n",
        "                        # if theoretical missing, consider ICSD presence as \"observed\"\n",
        "                        val = bool(icsd_ids)\n",
        "                    df.loc[df[\"bulk_id\"] == mid, \"experimentally_observed\"] = val\n",
        "                except Exception:\n",
        "                    # leave as NA if anything goes wrong\n",
        "                    pass\n",
        "\n",
        "    # cast to boolean where possible\n",
        "    df[\"experimentally_observed\"] = df[\"experimentally_observed\"].astype(\"boolean\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "N8f60yxYX2gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = add_experimentally_observed(df,  API_KEY)"
      ],
      "metadata": {
        "id": "3bQ9cqVLX9Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "7-AFs7e7YKBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step2-label\"></a> Step 2 – Label “In‑Domain” vs “Out‑of‑Domain”\n"
      ],
      "metadata": {
        "id": "7enZwuFXy7po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"md‑in‑domain\"></span>1  Tag OC20 structures (in_domain flag)\n"
      ],
      "metadata": {
        "id": "nxh9kVzd-34u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds an in_domain boolean column to our master dataframe df."
      ],
      "metadata": {
        "id": "TyXBCVe---nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# 1. Load the pickled list of dicts\n",
        "with open('/content/bulks.pkl', 'rb') as f:\n",
        "    bulks = pickle.load(f)  # bulks\n",
        "\n",
        "# 2. Extract all src_id values\n",
        "#    We know each entry is a dict with key 'src_id'\n",
        "src_ids = {entry['src_id'] for entry in bulks if 'src_id' in entry}  # list-of-dicts\n",
        "\n",
        "# 3. Flag DataFrame rows whose bulk_id is in that set\n",
        "df['in_domain'] = df['bulk_id'].isin(src_ids)\n",
        "\n",
        "# 4. Quick check\n",
        "print(f\"Found {len(src_ids)} unique src_id values in bulks.pkl\")\n",
        "print(df[['bulk_id', 'in_domain']].head(10))\n"
      ],
      "metadata": {
        "id": "EAtHuzvhz5Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "rVIio91X3L2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"md‑sym‑stats\"></span>2  Crystal‑system / space‑group statistics\n",
        "\n",
        "Here we group the dataframe by crystal_system and spacegroup to inspect\n",
        "\n",
        "*   average unit‑cell volume (Mean Volume),\n",
        "*   population count (Count), and\n",
        "*   the list of bulk IDs that fall into each symmetry bin.\n",
        "\n"
      ],
      "metadata": {
        "id": "ycV9CA3S_EpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by crystal system and symbol\n",
        "grouped = df.groupby(['crystal_system', 'spacegroup'], sort=False)\n",
        "\n",
        "# Perform aggregations: mean and count of volume, and list out bulk_ids\n",
        "aggregated_data = grouped.agg({\n",
        "    'volume': ['mean', 'count'],\n",
        "    'bulk_id': lambda x: list(x)\n",
        "})\n",
        "\n",
        "# Rename columns for clarity\n",
        "aggregated_data.columns = ['Mean Volume', 'Count', 'Bulk IDs']\n",
        "\n",
        "# Sort by mean volume descending\n",
        "sorted_aggregated_data = aggregated_data.sort_values(by='Mean Volume', ascending=False)\n",
        "\n",
        "# Display the full aggregated table\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(sorted_aggregated_data)\n"
      ],
      "metadata": {
        "id": "wLilUK60eTHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symmetry–volume overview\n",
        "* Rare, low-symmetry lattices (e.g., triclinic **P-1**, monoclinic **C2/c**) show the **largest mean volumes** (> 700 Å³).  \n",
        "* Common high-symmetry groups (cubic **Fm-3 m**, hexagonal **P6₃/mmc**) cluster at **smaller volumes** (≈ 60–135 Å³) and dominate the dataset."
      ],
      "metadata": {
        "id": "eMXovQeT_-s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['spacegroup'].value_counts().head(10)"
      ],
      "metadata": {
        "id": "b-Lql8qoeTJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['crystal_system'].value_counts().head(5)"
      ],
      "metadata": {
        "id": "DUPvLpHweTME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we filter\n",
        "* Retain the five most frequent crystal systems: **Cubic, Hexagonal, Orthorhombic, Tetragonal, Trigonal**.  \n",
        "* Choose space groups that are (i) well-populated, (ii) span P/I/F centering, and (iii) keep **mean volumes < 135 Å³**."
      ],
      "metadata": {
        "id": "lXQSF1n2_iPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "selected_spacegroups = [\n",
        "    # ── cubic ──\n",
        "    \"Pm-3m\", \"Fm-3m\", \"Fd-3m\", \"Pm-3n\", \"Im-3m\",\n",
        "    # ── orthorhombic ──\n",
        "    \"Cmmm\", \"Pmmn\", \"Pmma\", \"Fmmm\", \"Immm\", \"Pnnm\",\n",
        "    # ── tetragonal ──\n",
        "    \"I4/mmm\", \"P4/mmm\", \"I4/mcm\",\n",
        "    # ── hexagonal ──\n",
        "    \"P6_3/mmc\",\n",
        "    \"P-6m2\",\n",
        "    # ── trigonal ──\n",
        "    \"R-3m\",\n",
        "]\n",
        "\n",
        "df_selected = df[\n",
        "    df[\"spacegroup\"].isin(selected_spacegroups)\n",
        "].copy()\n",
        "\n",
        "df_selected.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "display(df_selected.head())"
      ],
      "metadata": {
        "id": "BFF-aBqxeTO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EG1PnZwM_qVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_in = df_selected[df_selected[\"in_domain\"]].copy()"
      ],
      "metadata": {
        "id": "dvwqGvyNeTSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in['band_gap'].value_counts()"
      ],
      "metadata": {
        "id": "q1Pe8DMeeTXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in['formation_energy_per_atom'].hist()"
      ],
      "metadata": {
        "id": "KoRUGGgB4bW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"md‑save‑subset\"></span>4  Save OC20‑matching structures (in_domain.pkl)\n",
        "This final cell pulls the actual bulk structures (ASE Atoms objects) that\n",
        "correspond to the IDs in df_in, then saves the subset as in_domain.pkl"
      ],
      "metadata": {
        "id": "0QUAhdYoCLxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- paths ---\n",
        "OUT_PKL   = \"/content/in_domain.pkl\"   # output\n",
        "\n",
        "# --- 1) get the whitelist of IDs from df_in ---\n",
        "assert \"bulk_id\" in df_in.columns, \"df_in must have a 'bulk_id' column\"\n",
        "allowed_ids = set(df_in[\"bulk_id\"].astype(str).unique())\n",
        "\n",
        "\n",
        "# --- 3) filter by src_id ∈ allowed_ids ---\n",
        "in_domain = [entry for entry in bulks\n",
        "             if isinstance(entry, dict) and str(entry.get(\"src_id\")) in allowed_ids]\n",
        "\n",
        "# --- 4) save subset ---\n",
        "with open(OUT_PKL, \"wb\") as f:\n",
        "    pickle.dump(in_domain, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"bulks total:   {len(bulks)}\")\n",
        "print(f\"unique IDs in df_in: {len(allowed_ids)}\")\n",
        "print(f\"kept (in-domain): {len(in_domain)}\")\n",
        "print(f\"saved → {OUT_PKL}\")\n"
      ],
      "metadata": {
        "id": "Mm-OOHcteTT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OUT-OF-DOMAIN Bulk Data Set Analysis"
      ],
      "metadata": {
        "id": "XHiU5xGZJB3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = df_selected[~df_selected[\"in_domain\"]].copy()"
      ],
      "metadata": {
        "id": "Z66bMPae4bcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step3-enrich\"></a>  Enrich Missing Symmetry & Properties from OC20 pickle file\n",
        "\n",
        "The initial **OC20** `bulks.pkl` catalogue occasionally contains entries whose Materials Project (MP) records are incomplete or stale.  \n",
        "To create a **robust and up‑to‑date out‑of‑domain dataset**, we therefore run a two–pronged enrichment step:\n",
        "\n",
        "1. **Local symmetry prediction** for structures lacking crystallographic metadata.  \n",
        "2. **Composition‑based data filling** for any residual thermodynamic / electronic gaps.\n",
        "\n",
        "The code cell below implements the complete workflow.\n",
        "\n",
        "---\n",
        "\n",
        "#### <a name=\"subsec-symmetry-pred\"></a> Predict Symmetry Locally\n",
        "Crystallographic symmetry is pivotal for classifying and rationalising materials.  \n",
        "For any entry missing *space group* or *crystal system*, we call **`predict_symmetry`**, which:\n",
        "\n",
        "* converts the ASE `Atoms` object to a `pymatgen` **`Structure`**,  \n",
        "* invokes `SpacegroupAnalyzer` at a tolerance of `symprec = 5 × 10⁻³`,  \n",
        "* returns the inferred `crystal_system`, `space_group`, and `point_group`.\n",
        "\n",
        "This guarantees a consistent structural description across the full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### <a name=\"subsec-gap-fill\"></a> Fill Numeric Gaps by Composition Match\n",
        "Key thermodynamic and electronic quantities—**formation energy**, **band gap**, **volume**—may still be absent for certain `bulk_id`s.  \n",
        "The helper **`fill_missing_props`** addresses this as follows:\n",
        "\n",
        "1. Query MP for **all polymorphs** sharing the *exact chemical formula* and *identical symmetry* (space group or point group).  \n",
        "2. Rank candidates by **`energy_above_hull`** and choose the *most stable* structure.  \n",
        "3. Transfer its formation energy, band gap, and volume to the target entry.  \n",
        "4. Propagate the `theoretical` flag so that `experimentally_observed` is updated consistently.\n",
        "\n",
        "This chemically intuitive heuristic ensures every composition is represented by the **thermodynamically most favourable** data available.\n"
      ],
      "metadata": {
        "id": "J_BMCE5XFAIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0) helpers ---------------------------------------------------------------\n",
        "\n",
        "FIELDS_MAIN = [\n",
        "    \"material_id\",\n",
        "    \"formation_energy_per_atom\", \"band_gap\", \"volume\",\n",
        "    \"symmetry.crystal_system\", \"symmetry.symbol\", \"symmetry.point_group\",\n",
        "    \"theoretical\",                  # ← will drive experimentally_observed\n",
        "]\n",
        "\n",
        "COL_MAP = {\n",
        "    \"formation_energy_per_atom\": \"formation_energy_per_atom\",\n",
        "    \"band_gap\":                   \"band_gap\",\n",
        "    \"volume\":                     \"volume\",\n",
        "    \"symmetry.crystal_system\":    \"crystal_system\",\n",
        "    \"symmetry.symbol\":            \"space_group\",\n",
        "    \"symmetry.point_group\":       \"point_group\",\n",
        "    \"theoretical\":                \"theoretical\",\n",
        "}\n",
        "\n",
        "def set_observed_from_theoretical(df):\n",
        "    \"\"\"Create/refresh 'experimentally_observed' from 'theoretical'.\"\"\"\n",
        "    if \"experimentally_observed\" not in df.columns:\n",
        "        df[\"experimentally_observed\"] = None\n",
        "    mask = df[\"theoretical\"].notna()\n",
        "    df.loc[mask, \"experimentally_observed\"] = ~df.loc[mask, \"theoretical\"].astype(bool)\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- 1) build df_filtered (unchanged) ----------------------------------------\n",
        "\n",
        "symbols = [\"Ir\",\"Pt\",\"Ru\",\"Ni\",\"Co\",\"Fe\",\"Mn\",\"Ta\",\"Ti\",\"Nb\",\"Mo\",\"Cu\",\"Sn\",\"Sb\"]\n",
        "valid_pairs = {tuple(sorted(p)) for p in combinations(symbols, 2)}\n",
        "\n",
        "filtered = []\n",
        "for entry in bulks:\n",
        "    atoms = entry.get(\"atoms\")\n",
        "    if isinstance(atoms, Atoms):\n",
        "        elems = sorted(set(atoms.get_chemical_symbols()))\n",
        "        if len(elems) == 2 and tuple(elems) in valid_pairs:\n",
        "            filtered.append({\n",
        "                \"chemsys\": f\"{elems[0]}-{elems[1]}\",\n",
        "                \"bulk_id\": entry[\"src_id\"],\n",
        "                \"formula\": atoms.get_chemical_formula(),\n",
        "                \"atoms\":   atoms,\n",
        "            })\n",
        "\n",
        "df_filtered = pd.DataFrame(filtered)\n",
        "\n",
        "# pre-create columns we will fill\n",
        "for out_col in COL_MAP.values():\n",
        "    if out_col not in df_filtered.columns:\n",
        "        df_filtered[out_col] = None\n",
        "\n",
        "\n",
        "# --- 2) fetch properties in batches (includes 'theoretical') -----------------\n",
        "\n",
        "def fetch_props_for_ids(df, api_key, batch_size=40):\n",
        "    with MPRester(api_key) as mpr:\n",
        "        ids = df[\"bulk_id\"].tolist()\n",
        "        for i in range(0, len(ids), batch_size):\n",
        "            batch = ids[i:i+batch_size]\n",
        "            try:\n",
        "                docs = mpr.materials.summary.search(\n",
        "                    material_ids=batch,\n",
        "                    fields=FIELDS_MAIN,\n",
        "                    chunk_size=len(batch)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                # retry one-by-one if a server timeout happens\n",
        "                for mid in batch:\n",
        "                    try:\n",
        "                        dlist = mpr.materials.summary.search(\n",
        "                            material_ids=[mid],\n",
        "                            fields=FIELDS_MAIN,\n",
        "                            chunk_size=1\n",
        "                        )\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    docs.extend(dlist)\n",
        "\n",
        "            for doc in docs:\n",
        "                idx = df.index[df[\"bulk_id\"] == doc.material_id]\n",
        "                if len(idx) == 0:\n",
        "                    continue\n",
        "                j = idx[0]\n",
        "\n",
        "                for api_f, df_f in COL_MAP.items():\n",
        "                    if \".\" in api_f:\n",
        "                        p0, p1 = api_f.split(\".\")\n",
        "                        parent = getattr(doc, p0, None)\n",
        "                        val = getattr(parent, p1, None) if parent is not None else None\n",
        "                    else:\n",
        "                        val = getattr(doc, api_f, None)\n",
        "\n",
        "                    if val is not None:\n",
        "                        df.at[j, df_f] = val\n",
        "\n",
        "    # derive 'experimentally_observed'\n",
        "    return set_observed_from_theoretical(df)\n",
        "\n",
        "df_filtered = fetch_props_for_ids(df_filtered, API_KEY)\n",
        "\n",
        "\n",
        "# --- 3) predict symmetry where missing (unchanged) ---------------------------\n",
        "\n",
        "def predict_symmetry(atoms):\n",
        "    struct = AseAtomsAdaptor.get_structure(atoms)\n",
        "    sga = SpacegroupAnalyzer(struct, symprec=5e-3, angle_tolerance=5.0)\n",
        "    return {\n",
        "        \"crystal_system\": sga.get_crystal_system().capitalize(),\n",
        "        \"space_group\":    sga.get_space_group_symbol(),\n",
        "        \"point_group\":    sga.get_point_group_symbol(),\n",
        "    }\n",
        "\n",
        "need_sym = df_filtered[\"space_group\"].isna() | df_filtered[\"point_group\"].isna()\n",
        "for idx in df_filtered[need_sym].index:\n",
        "    atoms = df_filtered.at[idx, \"atoms\"]\n",
        "    if atoms is None:\n",
        "        continue\n",
        "    try:\n",
        "        sym = predict_symmetry(atoms)\n",
        "        for k, v in sym.items():\n",
        "            df_filtered.at[idx, k] = v\n",
        "    except Exception as e:\n",
        "        print(f\"Symmetry prediction failed for {df_filtered.at[idx, 'bulk_id']}: {e}\")\n",
        "\n",
        "\n",
        "# --- 4) fill remaining numeric props by composition match --------------------\n",
        "\n",
        "from pymatgen.core import Composition\n",
        "\n",
        "def fill_missing_props(df, api_key):\n",
        "    TARGET = [\"formation_energy_per_atom\", \"band_gap\", \"volume\"]\n",
        "    df[TARGET] = df[TARGET].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    miss = df[TARGET].isna().any(axis=1)\n",
        "    if not miss.any():\n",
        "        return set_observed_from_theoretical(df)   # ensure observed set\n",
        "\n",
        "    with MPRester(api_key) as mpr:\n",
        "        for idx, row in df[miss].iterrows():\n",
        "            formula = row.get(\"formula\")\n",
        "            if not isinstance(formula, str) or pd.isna(formula):\n",
        "                continue\n",
        "            sym_ref = row.get(\"space_group\") or row.get(\"point_group\")\n",
        "            if not sym_ref:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                comp = Composition(formula)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            docs = mpr.materials.summary.search(\n",
        "                formula=formula,\n",
        "                num_elements=len(comp.elements),\n",
        "                fields=TARGET + [\"symmetry\", \"energy_above_hull\", \"composition\", \"theoretical\"],\n",
        "                chunk_size=300,\n",
        "            )\n",
        "\n",
        "            best = None\n",
        "            best_eh = float(\"inf\")\n",
        "            for d in docs:\n",
        "                if d.composition is None or d.symmetry is None:\n",
        "                    continue\n",
        "                try:\n",
        "                    if Composition(d.composition) != comp:\n",
        "                        continue\n",
        "                except Exception:\n",
        "                    continue\n",
        "                if d.symmetry.symbol == sym_ref or d.symmetry.point_group == sym_ref:\n",
        "                    eh = d.energy_above_hull if d.energy_above_hull is not None else float(\"inf\")\n",
        "                    if eh < best_eh:\n",
        "                        best, best_eh = d, eh\n",
        "\n",
        "            if best is not None:\n",
        "                df.at[idx, \"formation_energy_per_atom\"] = best.formation_energy_per_atom\n",
        "                df.at[idx, \"band_gap\"] = best.band_gap\n",
        "                df.at[idx, \"volume\"] = best.volume\n",
        "                # also set theoretical → observed if we didn’t have it\n",
        "                if pd.isna(df.at[idx, \"theoretical\"]) and getattr(best, \"theoretical\", None) is not None:\n",
        "                    df.at[idx, \"theoretical\"] = bool(best.theoretical)\n",
        "\n",
        "    # refresh observed column\n",
        "    return set_observed_from_theoretical(df)\n",
        "\n",
        "df_filtered = fill_missing_props(df_filtered, API_KEY)\n",
        "\n",
        "print(\"Done. df_filtered shape:\", df_filtered.shape)\n"
      ],
      "metadata": {
        "id": "Nr-HyO4V4bfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.columns"
      ],
      "metadata": {
        "id": "aL77Kjwr4biD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overwrite df_filtered with tidy columns (atoms last)\n",
        "df_filtered = (\n",
        "    df_filtered.rename(columns={\"formula\": \"formula_pretty\", \"space_group\": \"spacegroup\"})\n",
        ")\n",
        "\n",
        "# add any missing columns with NA\n",
        "for col in [\"density\", \"energy_above_hull\", \"in_domain\", \"atoms\"]:\n",
        "    if col not in df_filtered.columns:\n",
        "        df_filtered[col] = pd.NA\n",
        "\n",
        "# drop columns you don't want\n",
        "if \"point_group\" in df_filtered.columns:\n",
        "    df_filtered = df_filtered.drop(columns=[\"point_group\"])\n",
        "\n",
        "# final order\n",
        "order = [\n",
        "    \"chemsys\",\n",
        "    \"bulk_id\",\n",
        "    \"formula_pretty\",\n",
        "    \"crystal_system\",\n",
        "    \"spacegroup\",\n",
        "    \"volume\",\n",
        "    \"formation_energy_per_atom\",\n",
        "    \"band_gap\",\n",
        "    \"experimentally_observed\",\n",
        "    \"atoms\"]\n",
        "df_filtered = df_filtered.reindex(columns=order)\n"
      ],
      "metadata": {
        "id": "xOz8_mKeDfFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered"
      ],
      "metadata": {
        "id": "onjfZrVXDMoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_spacegroups = [\n",
        "    # cubic\n",
        "    \"Pm-3m\", \"Fm-3m\", \"Fd-3m\", \"Pm-3n\", \"Im-3m\",\n",
        "    # orthorhombic\n",
        "    \"Cmmm\", \"Pmmn\", \"Pmma\", \"Fmmm\", \"Immm\", \"Pnnm\",\n",
        "    # tetragonal\n",
        "    \"I4/mmm\", \"P4/mmm\", \"I4/mcm\",\n",
        "    # hexagonal\n",
        "    \"P6_3/mmc\",\n",
        "    \"P-6m2\",\n",
        "    # trigonal\n",
        "    \"R-3m\",\n",
        "]\n",
        "\n",
        "# 1)  all matching rows\n",
        "df_filtered_selected = df_filtered[\n",
        "    df_filtered[\"spacegroup\"].isin(selected_spacegroups)\n",
        "].copy()\n",
        "\n",
        "# Preview\n",
        "display(df_filtered_selected.head())"
      ],
      "metadata": {
        "id": "TS5uigzYbvnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify new entries by bulk_id\n",
        "new_entries = df_filtered_selected[~df_filtered_selected['bulk_id'].isin(df_in['bulk_id'])]\n",
        "\n",
        "# Step 2: Prepare new entries for merging\n",
        "# Add missing columns and set in_domain to False\n",
        "new_entries = new_entries.assign(in_domain=False)\n",
        "\n",
        "# Step 3: Align columns with df_out\n",
        "# Select only the columns present in df_out\n",
        "df_out_columns = df_out.columns.tolist()\n",
        "new_entries = new_entries[df_out_columns]\n",
        "\n",
        "# Step 4: Merge with df_out\n",
        "df_out_updated = pd.concat([df_out, new_entries], ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Added {len(new_entries)} new entries to the dataset\")\n",
        "print(f\"Updated dataset now has {len(df_out_updated)} entries\")"
      ],
      "metadata": {
        "id": "rY2hfxcOKsxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure that our out‑of‑domain dataset remains consistent with the objectives of this study, we remove a small number of Materials Project entries that do not meet our bulk‑screening criteria:\n",
        "\n",
        "- **mp‑1213433**  \n",
        "  This entry corresponds to a two‑dimensional Cu–Pt layered structure rather than a true 3D bulk phase. Such quasi‑2D configurations fall outside the scope of our bulk stability analysis and are thus excluded.  \n",
        "\n",
        "- **mp‑1208216**, **mp‑1537768**: These compositions were uploaded to the Materials Project after our initial screening cutoff and have not been characterized within our target dataset. To maintain consistency with our article’s data format and avoid introducing newly added, unvalidated structures, we therefore exclude them from df_out_updated.\n",
        "\n",
        "\n",
        "The following code cell applies this exclusion filter:"
      ],
      "metadata": {
        "id": "VKNGzwDCVOmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set of MPID objects to drop\n",
        "remove_set = {\"mp-1208216\", \"mp-1213433\", \"mp-1537768\"}\n",
        "\n",
        "df_out_updated = df_out_updated[~df_out_updated['bulk_id'].isin(remove_set)]"
      ],
      "metadata": {
        "id": "XSpPXvQcK6J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "R24dRAUULoHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MPRester with your API key\n",
        "\n",
        "mpr = MPRester(API_KEY)\n",
        "\n",
        "# Create a new column for atomic structures\n",
        "df_out_updated['atoms'] = None\n",
        "\n",
        "# Get unique bulk IDs that need atomic structures\n",
        "bulk_ids = df_out_updated['bulk_id'].unique().tolist()\n",
        "\n",
        "# Fetch structures in batches\n",
        "batch_size = 1000\n",
        "for i in range(0, len(bulk_ids), batch_size):\n",
        "    batch = bulk_ids[i:i+batch_size]\n",
        "\n",
        "    # Fetch material data with structures\n",
        "    docs = mpr.materials.summary.search(\n",
        "        material_ids=batch,\n",
        "        fields=[\"material_id\", \"structure\"]\n",
        "    )\n",
        "\n",
        "    # Create mapping from material ID to structure\n",
        "    structure_map = {doc.material_id: doc.structure for doc in docs}\n",
        "\n",
        "    # Convert structures to ASE Atoms and assign to dataframe\n",
        "    for idx, row in df_out_updated.iterrows():\n",
        "        if row['bulk_id'] in structure_map:\n",
        "            struct = structure_map[row['bulk_id']]\n",
        "            atoms = AseAtomsAdaptor.get_atoms(struct)\n",
        "            df_out_updated.at[idx, 'atoms'] = atoms"
      ],
      "metadata": {
        "id": "p9c12ig1LwcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create mapping dictionary from df_filtered_selected\n",
        "atoms_mapping = {}\n",
        "for _, row in df_filtered_selected.iterrows():\n",
        "    bulk_id = row['bulk_id']\n",
        "    atoms = row['atoms']\n",
        "    if not pd.isna(atoms) and atoms is not None:  # Ensure valid atoms object\n",
        "        atoms_mapping[bulk_id] = atoms\n",
        "\n",
        "# Step 2: Fill atoms from mapping dictionary\n",
        "def fill_atoms(row):\n",
        "    if (pd.isna(row['atoms']) or (row['atoms'] is None)):\n",
        "        return atoms_mapping.get(row['bulk_id'], None)\n",
        "    return row['atoms']\n",
        "\n",
        "df_out_updated['atoms'] = df_out_updated.apply(fill_atoms, axis=1)"
      ],
      "metadata": {
        "id": "NOWboZhHMaqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated.info()"
      ],
      "metadata": {
        "id": "nAjw5DCDMjUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "KTvV6llUMltl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "pHKbsypkXvPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in"
      ],
      "metadata": {
        "id": "l6QtNe_deDwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated.columns"
      ],
      "metadata": {
        "id": "PoJxEYsoeUV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step4-viz\"></a> Step 4 – Exploratory Visualisation\n"
      ],
      "metadata": {
        "id": "WnlzeSt4Kvjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import NullLocator\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "YCOL = \"formation_energy_per_atom\"\n",
        "XCAT = \"crystal_system\"\n",
        "HUE  = \"experimentally_observed\"      # boolean column\n",
        "\n",
        "DECOMP_ENERGY   = 0.2\n",
        "KINETIC_BARRIER = 0.5\n",
        "\n",
        "COL_OBS  = \"#08306B\"  # dark blue\n",
        "COL_NOBS = \"#67000D\"  # dark red\n",
        "EDGE     = \"black\"\n",
        "\n",
        "def _color_violins(vdict, face, edge=EDGE, alpha=0.85, lw=0.8):\n",
        "    for b in vdict[\"bodies\"]:\n",
        "        b.set_facecolor(face)\n",
        "        b.set_edgecolor(edge)\n",
        "        b.set_alpha(alpha)\n",
        "        b.set_linewidth(lw)\n",
        "\n",
        "def _panel(ax, df, ylim):\n",
        "    cats = sorted(df[XCAT].astype(str).unique())\n",
        "    pos  = np.arange(1, len(cats) + 1, dtype=float)\n",
        "    off  = 0.12\n",
        "\n",
        "    d_false = [df[(df[XCAT].astype(str) == c) & (~df[HUE])][YCOL].values for c in cats]\n",
        "    d_true  = [df[(df[XCAT].astype(str) == c) & ( df[HUE])][YCOL].values for c in cats]\n",
        "\n",
        "    v_false = ax.violinplot(d_false, positions=pos - off, widths=0.22,\n",
        "                            showmeans=False, showextrema=False, showmedians=False)\n",
        "    v_true  = ax.violinplot(d_true,  positions=pos + off, widths=0.22,\n",
        "                            showmeans=False, showextrema=False, showmedians=False)\n",
        "\n",
        "    _color_violins(v_false, COL_NOBS)\n",
        "    _color_violins(v_true,  COL_OBS)\n",
        "\n",
        "    ax.axhline(DECOMP_ENERGY,   linestyle=\"--\", linewidth=1.2, alpha=0.9)\n",
        "    ax.axhline(KINETIC_BARRIER, linestyle=\"--\", linewidth=1.2, alpha=0.9)\n",
        "\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.set_xlim(0.5, len(cats) + 0.5)\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.25, zorder=0)\n",
        "    ax.set_xlabel(\"Crystal System\")\n",
        "    ax.set_ylabel(\"Formation Energy (eV/atom)\")\n",
        "    ax.xaxis.set_minor_locator(NullLocator())\n",
        "    ax.yaxis.set_minor_locator(NullLocator())\n",
        "    ax.tick_params(axis=\"x\", which=\"major\", direction=\"out\", length=8, width=2, rotation=45)\n",
        "    ax.tick_params(axis=\"y\", which=\"major\", direction=\"out\", length=8, width=2)\n",
        "    for sp in (\"bottom\", \"left\"):\n",
        "        ax.spines[sp].set_linewidth(2)\n",
        "\n",
        "    # ≤5 evenly spaced tick labels\n",
        "    if len(cats) <= 5:\n",
        "        tick_pos, tick_lab = pos, cats\n",
        "    else:\n",
        "        idx = sorted({int(round(i)) for i in np.linspace(0, len(cats) - 1, 5)})[:5]\n",
        "        tick_pos = pos[idx]\n",
        "        tick_lab = [cats[i] for i in idx]\n",
        "    ax.set_xticks(tick_pos, tick_lab)\n",
        "    for lab in ax.get_xticklabels():\n",
        "        lab.set_ha(\"right\")\n",
        "\n",
        "# ---- Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "_panel(ax1, df_in,           ylim=(-1.25, 0.60))\n",
        "_panel(ax2, df_out_updated,  ylim=(-1.25, 1.25))\n",
        "\n",
        "ax1.set_title(\"In Domain Structures\")\n",
        "ax2.set_title(\"Out of Domain Structures\")\n",
        "\n",
        "# --- Annotations on left panel\n",
        "ax1.text(0.98, KINETIC_BARRIER, \"Kinetically\\nAccessible\",\n",
        "         fontsize=14, fontweight=\"bold\", ha=\"left\", va=\"center\",\n",
        "         transform=ax1.get_yaxis_transform(which=\"grid\"))\n",
        "ax1.text(0.98, DECOMP_ENERGY, \"Thermodynamically\\nStable\",\n",
        "         fontsize=14, fontweight=\"bold\", ha=\"left\", va=\"center\",\n",
        "         transform=ax1.get_yaxis_transform(which=\"grid\"))\n",
        "\n",
        "# shared legend\n",
        "handles = [Patch(facecolor=COL_OBS,  edgecolor=EDGE, label=\"Observed\"),\n",
        "           Patch(facecolor=COL_NOBS, edgecolor=EDGE, label=\"Not Observed\")]\n",
        "fig.legend(handles=handles, loc=\"upper center\", ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "base = \"/content/formation_energy_stability\"\n",
        "for ext in (\"pdf\", \"png\", \"tiff\"):\n",
        "    plt.savefig(f\"{base}.{ext}\", dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z-h2h1h8eVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated['bulk_id'].nunique()"
      ],
      "metadata": {
        "id": "xbY3lLDfijLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated"
      ],
      "metadata": {
        "id": "CUgWHrrYmRHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step5-vasp\"></a> Step 5 – Generate VASP Input Decks\n"
      ],
      "metadata": {
        "id": "I0-yCZ5VK0LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ⚠️ **VASP license required**  \n",
        "> The next cell writes full VASP input folders from each `ase.Atoms` object.  \n",
        "> You must be an **authorized VASP user** with access to PAW-PBE POTCARs; otherwise the cell will fail and you may violate the VASP EULA.\n",
        "\n",
        "| File        | Purpose (edit paths marked **EDIT HERE**) |\n",
        "|-------------|-------------------------------------------|\n",
        "| `POSCAR`    | Bulk geometry from `ase.Atoms`. |\n",
        "| `KPOINTS`   | Fixed Γ-centered 10 × 10 × 10 mesh. |\n",
        "| `INCAR`     | Generic bulk‐relax settings (`BULK_VASP_FLAGS`). |\n",
        "| `POTCAR`    | Concatenated from `<POTCAR_DIR>/<elem>/POTCAR`. |\n",
        "| `job.sh`    | Example SLURM script — adapt queue, time, modules. |\n",
        "\n",
        "**Customise before running**\n",
        "\n",
        "1. **`BASE_DIR`** – where run folders are created *(EDIT HERE)*.  \n",
        "2. **`POTCAR_DIR`** – root of your pseudopotential library *(EDIT HERE)*.  \n",
        "3. `write_batch_script()` – tweak `#SBATCH` lines for your HPC.  \n",
        "\n",
        "The helper routines are cluster-agnostic; only path strings and the SLURM header are site-specific.\n",
        "\n",
        "After relaxation, results are uploaded to Google Drive; the notebook then parses each `OUTCAR`, converts the relaxed structures back to pickled `ase.Atoms`, and feeds them into the downstream MLIP relaxation sub-routine.  \n",
        "A full end-to-end demonstration is available in the companion tutorial:  \n",
        "<https://github.com/ergroup/EqV2-HER-Discovery/blob/main/notebooks/demo_InDomain_H_ads.ipynb>\n"
      ],
      "metadata": {
        "id": "4CpmtnzHNu4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 1) VASP flags for OC20 bulk (check the github repository https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/utils/vasp.py)\n",
        "# ----------------------------------------------------------------------------------\n",
        "BULK_VASP_FLAGS = {\n",
        "    \"ibrion\": 1,         # ionic optimisation (quasi‑Newton)\n",
        "    \"nsw\": 100,          # max ionic steps\n",
        "    \"isif\": 7,           # relax both ions and cell\n",
        "    \"isym\": 0,           # symmetry off (safer for low‑symmetry structures)\n",
        "    \"ediffg\": 1e-8,      # force‑convergence criterion\n",
        "    \"encut\": 500.0,      # plane‑wave cutoff (eV)\n",
        "    \"kpts\": (10, 10, 10),\n",
        "    \"prec\": \"Accurate\",\n",
        "    \"gga\": \"RP\",\n",
        "    \"pp\":  \"PBE\",\n",
        "    \"lwave\":  False,\n",
        "    \"lcharg\": False,\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 2) File‑writer helpers\n",
        "# ----------------------------------------------------------------------------------\n",
        "def write_poscar(atoms: Atoms, folder: str):\n",
        "    \"\"\"POSCAR from ASE Atoms.\"\"\"\n",
        "    write(Path(folder, \"POSCAR\"), atoms, format=\"vasp\")\n",
        "\n",
        "def write_kpoints(folder: str):\n",
        "    k0, k1, k2 = BULK_VASP_FLAGS[\"kpts\"]\n",
        "    Path(folder, \"KPOINTS\").write_text(\n",
        "        f\"Automatic mesh\\n0\\nMonkhorst–Pack\\n{k0} {k1} {k2}\\n0 0 0\\n\"\n",
        "    )\n",
        "\n",
        "def create_potcar(folder: str, potcar_root: str):\n",
        "    \"\"\"\n",
        "    Concatenate POTCAR fragments – requires\n",
        "    `<potcar_root>/<Element>/POTCAR` for every species in POSCAR.\n",
        "    \"\"\"\n",
        "    elems = Path(folder, \"POSCAR\").read_text().splitlines()[5].split()\n",
        "    txt = []\n",
        "    for Z in elems:\n",
        "        psrc = Path(potcar_root, Z, \"POTCAR\")\n",
        "        if not psrc.is_file():\n",
        "            raise FileNotFoundError(f\"No POTCAR for {Z} → {psrc}\")\n",
        "        txt.append(psrc.read_text())\n",
        "    Path(folder, \"POTCAR\").write_text(\"\".join(txt))\n",
        "\n",
        "def write_incar(folder: str):\n",
        "    lines = []\n",
        "    for k, v in BULK_VASP_FLAGS.items():\n",
        "        v_fmt = \".TRUE.\" if v is True else \".FALSE.\" if v is False else v\n",
        "        lines.append(f\"{k.upper()} = {v_fmt}\")\n",
        "    Path(folder, \"INCAR\").write_text(\"\\n\".join(lines))\n",
        "\n",
        "def write_batch_script(folder: str):\n",
        "    \"\"\"\n",
        "    Example SLURM launcher  – **edit for your HPC environment**.\n",
        "    \"\"\"\n",
        "    job = Path(folder).name\n",
        "    Path(folder, \"job.sh\").write_text(f\"\"\"#!/bin/bash\n",
        "#SBATCH -N xx\n",
        "#SBATCH -n xx\n",
        "#SBATCH --time=xx:00:00       # <<< EDIT HERE\n",
        "#SBATCH -p xxx              # <<< EDIT HERE\n",
        "#SBATCH -J {job}\n",
        "#SBATCH --output=out.%j\n",
        "#SBATCH --error=err.%j\n",
        "\n",
        "module load xx             # <<< EDIT HERE\n",
        "module load VASP5/5.4.4.xx  # <<< EDIT HERE\n",
        "srun vasp_std > vasp.out\n",
        "\"\"\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 3) Driver – iterate over DataFrame rows\n",
        "# ----------------------------------------------------------------------------------\n",
        "def generate_vasp_inputs(df, base_dir, potcar_dir):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Must contain columns 'bulk_id' and 'atoms'.\n",
        "    base_dir : str or Path\n",
        "        Parent directory where <bulk_id>/ folders will be created.\n",
        "    potcar_dir : str or Path\n",
        "        Root folder containing element‑wise POTCAR sub‑dirs.\n",
        "    \"\"\"\n",
        "    base_dir  = Path(base_dir).expanduser().resolve()\n",
        "    potcar_dir = Path(potcar_dir).expanduser().resolve()\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        bid   = row[\"bulk_id\"]\n",
        "        atoms = row[\"atoms\"]\n",
        "        work  = base_dir / bid\n",
        "        work.mkdir(exist_ok=True)\n",
        "\n",
        "        write_poscar(atoms, work)\n",
        "        write_kpoints(work)\n",
        "        create_potcar(work, potcar_dir)\n",
        "        write_incar(work)\n",
        "        write_batch_script(work)\n",
        "\n",
        "        print(f\"✔  VASP inputs written → {work}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 4) Example call  (uncomment & adapt paths)\n",
        "# ----------------------------------------------------------------------------------\n",
        "# BASE_DIR   = \"/path/to/vasp_runs/out_of_domain\"   # <<< EDIT HERE\n",
        "# POTCAR_DIR = \"/path/to/pseudopotentials\"          # <<< EDIT HERE\n",
        "# generate_vasp_inputs(df_out_updated, BASE_DIR, POTCAR_DIR)"
      ],
      "metadata": {
        "id": "ODhibkWeOhgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-outcar-extract\"></a>Step 6 – Build the **Out‑of‑Domain** Bulk Pickle from VASP *OUTCAR*s  \n",
        "This cell harvests **fully‑relaxed bulk geometries and their single‑point energies** from VASP calculations that were executed for the out‑of‑domain candidates identified earlier.\n",
        "\n",
        "**Workflow**\n",
        "\n",
        "1. **User parameters**  \n",
        "   * `base_directory` – root folder that contains one sub‑directory per material (`mp‑*`), each holding an `OUTCAR`.  \n",
        "   * `output_pickle`  – target path for the consolidated pickle (`OutOfDomainBulk.pkl`).  \n",
        "   * `sampling_template` – optional string for attaching an arbitrary provenance tag (`bulk_sampling_str`).\n",
        "\n",
        "2. **Iterate through `mp‑*/OUTCAR` files**  \n",
        "   * Skip folders without a valid `OUTCAR`.  \n",
        "   * Read the **final frame** (`index = -1`) with `ase.io.read`, which attaches a `SinglePointCalculator` containing the converged total energy, forces, and stress.\n",
        "\n",
        "3. **Integrity checks**  \n",
        "   * Verify that the attached calculator is indeed an `ase.calculators.singlepoint.SinglePointCalculator`.  \n",
        "   * Emit warnings for missing or malformed outputs without interrupting the loop.\n",
        "\n",
        "4. **Collect metadata**  \n",
        "   * Store `src_id` (folder name), a random `bulk_sampling_str` (for traceability), and the full `Atoms` object (including the calculator).\n",
        "\n",
        "5. **Serialise**  \n",
        "   * Dump the accumulated list to `output_pickle` with `pickle.HIGHEST_PROTOCOL`.\n",
        "\n",
        "**Outcome**  \n",
        "A single file — `OutOfDomainBulk.pkl` — that bundles every out‑of‑domain bulk structure together with its DFT total energy, ready for downstream modelling or database ingestion.\n",
        "\n"
      ],
      "metadata": {
        "id": "zi4UWyVQK6GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── User parameters ─────────────────────────────────────────────────────────\n",
        "base_directory = \"/content/drive/MyDrive/OutOfDomain\"   # folder containing mp-*/OUTCAR\n",
        "output_pickle  = \"/content/OutOfDomainBulk.pkl\"\n",
        "sampling_template = \"{run}/{step}_{run2}/{step2}\"       # sampling\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "atoms_list = []\n",
        "\n",
        "for subdir in sorted(os.listdir(base_directory)):\n",
        "    if not subdir.startswith(\"mp-\"):\n",
        "        continue\n",
        "\n",
        "    outcar_path = os.path.join(base_directory, subdir, \"OUTCAR\")\n",
        "    if not os.path.isfile(outcar_path):\n",
        "        print(f\"⚠️  OUTCAR missing for {subdir}, skipping\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # read final frame with attached SinglePointCalculator\n",
        "        atoms = read(outcar_path, format=\"vasp-out\", index=-1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to read {outcar_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Sanity check: calculator should now be a SinglePointCalculator\n",
        "    calc = atoms.calc\n",
        "    if not isinstance(calc, SinglePointCalculator):\n",
        "        print(f\"⚠️  {subdir}: got calc={calc!r} (not SinglePointCalculator)\")\n",
        "\n",
        "    # build your sampling string however you like\n",
        "    sampling_str = sampling_template.format(\n",
        "        run    = random.randint(0,999),\n",
        "        step   = random.randint(0,9999),\n",
        "        run2   = random.randint(0,999),\n",
        "        step2  = random.randint(0,9999),\n",
        "    )\n",
        "\n",
        "    atoms_list.append({\n",
        "        \"src_id\":            subdir,\n",
        "        \"bulk_sampling_str\": sampling_str,\n",
        "        \"atoms\":             atoms,      # store the real Atoms w/ calculator\n",
        "    })\n",
        "    print(f\"✅ Loaded and stored {subdir}\")\n",
        "\n",
        "# write out the pickle\n",
        "with open(output_pickle, \"wb\") as f:\n",
        "    pickle.dump(atoms_list, f)\n",
        "\n",
        "print(f\"\\n🎉 Pickled {len(atoms_list)} structures → {output_pickle}\")\n"
      ],
      "metadata": {
        "id": "cfgot2Fx_Cb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step7-package\"></a> Step 7 – Package Merged Pickle\n"
      ],
      "metadata": {
        "id": "uatVlUWtQDrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Paths to your input pickles\n",
        "IN_DOMAIN_PKL       = \"in_domain.pkl\"\n",
        "OUT_OF_DOMAIN_PKL   = \"OutOfDomainBulk.pkl\"\n",
        "MERGED_PKL          = \"Allbulks.pkl\"\n",
        "\n",
        "# 1) Load both lists\n",
        "with open(IN_DOMAIN_PKL, \"rb\") as f:\n",
        "    in_domain = pickle.load(f)\n",
        "\n",
        "with open(OUT_OF_DOMAIN_PKL, \"rb\") as f:\n",
        "    out_domain = pickle.load(f)\n",
        "\n",
        "# 2) Combine and dedupe by src_id\n",
        "combined = in_domain + out_domain\n",
        "unique = {}\n",
        "for entry in combined:\n",
        "    src = entry.get(\"src_id\")\n",
        "    if src not in unique:\n",
        "        unique[src] = entry\n",
        "\n",
        "merged_list = list(unique.values())\n",
        "\n",
        "# 3) Save merged list\n",
        "with open(MERGED_PKL, \"wb\") as f:\n",
        "    pickle.dump(merged_list, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"Loaded {len(in_domain)} in‑domain + {len(out_domain)} out‑of‑domain entries\")\n",
        "print(f\"Merged (unique) entries: {len(merged_list)} → saved as {MERGED_PKL}\")\n"
      ],
      "metadata": {
        "id": "5k-1HgLiCA54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your pickled data\n",
        "pkl_path = '/content/Allbulks.pkl'\n",
        "\n",
        "# Load the list of dicts\n",
        "with open(pkl_path, 'rb') as f:\n",
        "    bulk_data = pickle.load(f)\n",
        "\n",
        "# Iterate and extract energy\n",
        "for entry in bulk_data:\n",
        "    src_id = entry.get('src_id')\n",
        "    atoms  = entry.get('atoms')\n",
        "    calc   = getattr(atoms, 'calc', None)\n",
        "\n",
        "    if isinstance(calc, SinglePointCalculator):\n",
        "        energy = calc.results.get('energy', None)\n",
        "    else:\n",
        "        energy = None\n",
        "\n",
        "    print(f\"src_id: {src_id}, Total Energy: {energy}\")\n"
      ],
      "metadata": {
        "id": "RJ2mPDXMSXhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated"
      ],
      "metadata": {
        "id": "LYO0CHrbO_Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣  remove the atoms column\n",
        "df_out_clean = df_out_updated.drop(columns=['atoms'])\n",
        "\n",
        "# 2️⃣  concatenate the two dataframes row-wise\n",
        "df_merged = pd.concat([df_in, df_out_clean], ignore_index=True)\n",
        "\n",
        "# 3️⃣  save to disk without the pandas index\n",
        "csv_name = \"bulk_in_out_merged.csv\"\n",
        "df_merged.to_csv(csv_name, index=False)\n",
        "\n",
        "print(f\"✓ Saved merged file → {csv_name}  (rows: {len(df_merged)})\")\n"
      ],
      "metadata": {
        "id": "qs-Wc9IaPD4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"sec-conclusions\"></a> Conclusions  \n",
        "\n",
        "This notebook demonstrates an **end‑to‑end, reproducible workflow** for discovering and analysing bimetallic bulk materials:\n",
        "\n",
        "1. **Data integration**  \n",
        "   *Queried* the Materials Project for every unique binary combination of fourteen transition‑ and post‑transition‑metal elements, then *cross‑referenced* those entries with the **Open Catalyst 2020** (OC20) bulks dataset.  \n",
        "   * Materials present in OC20 were labelled **in‑domain**; all others were classified **out‑of‑domain**.\n",
        "\n",
        "2. **Data enrichment & curation**  \n",
        "   * Filled missing crystal‑symmetry information via local `pymatgen` analysis when absent from the MP record.  \n",
        "   * Completed gaps in key thermodynamic/electronic properties by selecting the most stable polymorph (lowest _E<sub>hull</sub>_) with identical composition and symmetry.\n",
        "\n",
        "3. **Exploratory analysis**  \n",
        "   * Violin‑plot comparison of formation energies revealed systematic stability trends across crystal systems and highlighted differences between *experimentally observed* and *theoretical* compounds.\n",
        "\n",
        "4. **Down‑stream simulation readiness**  \n",
        "   * Automatically generated **VASP** input packages (POSCAR, INCAR, KPOINTS, POTCAR, batch script) for every out‑of‑domain structure, enabling immediate high‑throughput first‑principles calculations.\n",
        "\n",
        "---\n",
        "\n",
        "Together, these steps provide a practical template for **high‑throughput screening in catalysis and materials discovery**.  \n",
        "By combining large, curated databases with automated analysis and workflow tools, researchers can quickly target the most promising candidates for further experimental or computational investigation.\n",
        "\n"
      ],
      "metadata": {
        "id": "k1UBtfwlQIoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <a name=\"sec-references\"></a>9. References  \n",
        "\n",
        "##### 9.1 Data & Code  \n",
        "- **OC20 bulk structures** (`bulks.pkl`) – ocdata/databases/pkls/bulks.pkl, Open Catalyst Dataset. GitHub.  \n",
        "  https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/databases/pkls/bulks.pkl  \n",
        "\n",
        "- **VASP utility functions** (`vasp.py`) – ocdata/utils/vasp.py, Open Catalyst Dataset. GitHub.  \n",
        "  https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/utils/vasp.py  \n",
        "\n",
        "##### 9.2 Key Publications  \n",
        "- Chanussot, L.; Das, A.; Goyal, S.; Lavril, T.; Shuaibi, M.; Rivière, M.; Tran, K.; Heras‑Domingo, J.; Ho, C.; Hu, W.; Palizhati, A.; Sriram, A.; Wood, B.; Yoon, J.; Parikh, D.; Zitnick, C. L.; Ulissi, Z. Open Catalyst 2020 (OC20) Dataset and Community Challenges. *ACS Catalysis* **11**, 6059–6080 (2021). DOI: 10.1021/acscatal.0c04525  \n",
        "\n",
        "- Jain, A.; Ong, S. P.; Hautier, G.; Chen, W.; Richards, W. D.; Dacek, S.; Cholia, S.; Gunter, D.; Skinner, D.; Ceder, G.; Persson, K. A. Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. *APL Materials* **1**, 011002 (2013). DOI: 10.1063/1.4812323  \n",
        "\n"
      ],
      "metadata": {
        "id": "Gi1g98EaS36j"
      }
    }
  ]
}