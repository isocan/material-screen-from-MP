{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aYyLc7AC0JWLmkJsl4T-VpJ50rRVcNWg",
      "authorship_tag": "ABX9TyO7yzX7sJegLcU8dUYIFzPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isocan/material-screen-from-MP/blob/main/Material_Screening.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"sec-introduction\"></a>Â Introduction\n"
      ],
      "metadata": {
        "id": "B9WLhQ5_w7WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Material Screening Workflow\n",
        "\n",
        "This notebook demonstrates a systematic approach to materials screening using the Materials Project (MP) database. It specifically focuses on identifying bimetallic compounds, categorizing them as \"in-domain\" or \"out-of-domain\" based on their presence in the Open Catalyst 2020 (OC20) dataset, and enriching the data with additional computed properties from the Materials Project."
      ],
      "metadata": {
        "id": "Q-kTf7M8GWPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tableâ€¯ofâ€¯Contents  \n",
        "\n",
        "* **[1.â€¯Introduction](#sec-introduction)**\n",
        "* **[2.â€¯PrerequisitesÂ &Â Environment](#sec-env)**\n",
        "* **[3.â€¯Dataâ€¯Sources](#sec-data-sources)**\n",
        "* **[4.Â Stepâ€¯1Â â€“Â Query bimetallic entries](#sec-step1-query)**\n",
        "* **[5.Â Stepâ€¯2Â â€“Â Label â€œinâ€‘domainâ€ vs â€œoutâ€‘ofâ€‘domainâ€](#sec-step2-label)**\n",
        "* **[6.Â Stepâ€¯3Â â€“Â Enrich missing symmetry & properties](#sec-step3-enrich)**\n",
        "  * [6.1Â Predict symmetry locally](#subsec-symmetry-pred)\n",
        "  * [6.2Â Fill numeric gaps by composition match](#subsec-gap-fill)\n",
        "* **[7.Â Stepâ€¯4Â â€“Â Exploratory visualisation](#sec-step4-viz)**\n",
        "* **[8.Â Stepâ€¯5Â â€“Â Generate VASP input decks](#sec-step5-vasp)**\n",
        "* **[9.Â Stepâ€¯6Â â€“Â Build the Outâ€‘ofâ€‘Domain Bulk Pickle from VASP OUTCARs](#sec-outcar-extract)**\n",
        "* **[10.Â Stepâ€¯7Â â€“Â Package merged pickle](#sec-step7-package)**\n",
        "* **[11.â€¯Conclusions](#sec-conclusions)**\n",
        "* **[12. References](#sec-references)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WqUAXlrsGc9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-env\"></a>Â PrerequisitesÂ &Â Environment\n"
      ],
      "metadata": {
        "id": "QonvKTW8QRiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Verify Python environment"
      ],
      "metadata": {
        "id": "SgrRrxCbRG3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "_VK2DInpQQgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install and upgrade dependencies\n",
        "\n"
      ],
      "metadata": {
        "id": "NIbp-ksURdER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install pandas mp-api pymatgen ase tqdm matplotlib"
      ],
      "metadata": {
        "id": "Ow-HShy0Rban"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Import core libraries"
      ],
      "metadata": {
        "id": "_6x2h7UDRo3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Standard library\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from pathlib import Path              # file paths / saving results\n",
        "from itertools import combinations    # generate element pairs\n",
        "import pickle                         # serialize filtered bulks (in_domain.pkl)\n",
        "import ast                            # safe literal parsing (if needed for lists/tuples)\n",
        "import os\n",
        "import random\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Thirdâ€‘party scientific stack\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import numpy as np                    # numeric utilities\n",
        "import pandas as pd                   # tabular data wrangling\n",
        "from tqdm import tqdm                 # progress bars for API loops\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import NullLocator\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Atomistic / materials toolkits\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from ase import Atoms                                 # ASE structures in bulks.pkl\n",
        "from ase.io import read\n",
        "from ase.io import write\n",
        "from ase.calculators.singlepoint import PropertyNotImplementedError\n",
        "from ase.calculators.singlepoint import SinglePointCalculator\n",
        "from mp_api.client import MPRester                    # Materials Project REST client\n",
        "from pymatgen.core import Composition                 # chemistry utilities (parsing formulas)\n",
        "from pymatgen.io.ase import AseAtomsAdaptor           # ASEâ†”pymatgen structure conversion\n",
        "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer  # spaceâ€‘group detection\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ahMsD1KPX9Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> âš ï¸ **Materials Project API key required**  \n",
        "> Set it once per shell session  \n",
        "> `export MP_API_KEY=\"YOUR_KEY\"`  \n",
        "> **or** replace the placeholder below:  \n",
        "> ```python\n",
        "> # Replace with your Materials Project API key\n",
        "> API_KEY = \"XXXXXXXXX\"\n",
        "> ```\n"
      ],
      "metadata": {
        "id": "KOmJyOy-Mb94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-data-sources\"></a>Â DataÂ Sources\n"
      ],
      "metadata": {
        "id": "h5xB6j7mx5mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â¬‡ï¸â€¯Loading the OC20 *bulks.pkl* dataset  \n",
        "\n",
        "The **Openâ€¯Catalystâ€¯Project** distributes preâ€‘computed bulk crystal structures\n",
        "in a single pickle file, **`bulks.pkl`**.  \n",
        "Each element of the list stored in this file is a small Pythonâ€¯dictionary with:\n",
        "\n",
        "| key | description |\n",
        "|-----|-------------|\n",
        "| `atoms` | an **ASEâ€¯`Atoms`** object containing lattice vectors, atomic coordinates, andâ€”if availableâ€”a `SinglePointCalculator` with the final DFT results |\n",
        "| `src_id` | the canonical Materialsâ€¯Project identifier (e.g. `mpâ€‘1234`) |\n",
        "| `bulk_sampling_str` | OC20â€™s internal bookkeeping string for sampling provenance |\n",
        "\n",
        "You can download the file directly from the project repository:  \n",
        "<https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/databases/pkls/bulks.pkl>"
      ],
      "metadata": {
        "id": "NdwtRjGk1AvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ Load OC20 bulks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with open(\"/content/bulks.pkl\", \"rb\") as f:\n",
        "    bulks = pickle.load(f)   # â†’ list[dict]\n",
        "\n",
        "print(f\"Loaded {len(bulks):,} bulk structures from OC20.\")\n",
        "\n",
        "# Quick sanityâ€‘checks\n",
        "first = bulks[0]\n",
        "print(\"Keys in first entry:\", list(first.keys()))\n",
        "print(\"Example src_id:\", first['src_id'])\n",
        "print(\"ASE Atoms object:\", first['atoms'])"
      ],
      "metadata": {
        "id": "CWgjQs00xzXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”Â Bimetallicâ€pair statistics  \n",
        "The cell below scans the **`bulks`** list (loaded earlier) and counts how many\n",
        "times every unique twoâ€‘metal combination appears.  \n",
        "We then\n",
        "\n",
        "1.  list the **most common** and **least common** pairs, and  \n",
        "2.  draw a quick heatâ€‘map so you can spot dominant vs. sparse regions at a glance."
      ],
      "metadata": {
        "id": "xgUJ-4_F7dgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Parameters\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "symbols_of_interest = {\n",
        "    \"Ir\",\"Pt\",\"Ru\",\"Ni\",\"Fe\",\"Mo\",\"Ta\",\"Nb\",\"Ti\",\"Cu\",\"Sb\",\"Mn\",\"Co\",\"Sn\"\n",
        "}\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Build a DataFrame of all bimetal pairs in `bulks`\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "rows = []\n",
        "for entry in bulks:                           # `bulks` was loaded from bulks.pkl\n",
        "    atoms = entry[\"atoms\"]                   # ASEâ€¯Atoms\n",
        "    elems = sorted(set(atoms.get_chemical_symbols()))\n",
        "    # keep only pairs fully within our element list\n",
        "    if len(elems) == 2 and set(elems).issubset(symbols_of_interest):\n",
        "        rows.append({\"Ametal\": elems[0], \"Bmetal\": elems[1]})\n",
        "\n",
        "df_pairs = pd.DataFrame(rows)\n",
        "\n",
        "# count frequencies\n",
        "pair_counts = (\n",
        "    df_pairs\n",
        "    .groupby([\"Ametal\", \"Bmetal\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        "    .sort_values(\"count\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# â”€â”€â”€ 1) Text summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "TOP_N = 10\n",
        "print(f\"\\nğŸ” TopÂ {TOP_N} most frequent bimetallic pairs\")\n",
        "display(pair_counts.head(TOP_N))\n",
        "\n",
        "print(f\"\\nğŸ”» BottomÂ {TOP_N} least frequent pairs\")\n",
        "display(pair_counts.tail(TOP_N))\n",
        "\n",
        "# â”€â”€â”€ 2) Heatâ€‘map visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pivot = (\n",
        "    pair_counts\n",
        "    .pivot(index=\"Ametal\", columns=\"Bmetal\", values=\"count\")\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(pivot.values, cmap=\"viridis\")\n",
        "\n",
        "# axis labels and ticks\n",
        "ax.set_xticks(range(len(pivot.columns)))\n",
        "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
        "ax.set_yticks(range(len(pivot.index)))\n",
        "ax.set_yticklabels(pivot.index)\n",
        "\n",
        "# colourâ€‘bar\n",
        "cbar = fig.colorbar(im, ax=ax)\n",
        "cbar.set_label(\"Number of occurrences\")\n",
        "\n",
        "ax.set_title(\"Frequency heatâ€‘map of bimetallic pairs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uEjiQqak7IWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step1-query\"></a>Â Stepâ€¯1Â â€“Â QueryÂ BimetallicÂ Entries\n"
      ],
      "metadata": {
        "id": "-PU7nlQTy14V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ”„â€¯Highâ€‘throughput retrieval & labellingÂ of bimetallic entries  \n",
        "This code block:\n",
        "\n",
        "1. **Enumerates every unique binary combination** of the 14 transitionâ€‘metal / postâ€‘metal elements listed inÂ `symbols`.  \n",
        "2. **Queries the Materialsâ€¯Project** for each pair, requesting a compact but informationâ€‘rich set of fields (`FIELDS`).  \n",
        "3. **Builds a tidy pandas table** (`df`) with crystalâ€‘system, spaceâ€‘group, formationâ€‘energy, bandâ€‘gap and other key attributes.  \n",
        "4. **Adds anÂ `experimentally_observed` flag** â€“Â `True` when the MP record is tagged as *nonâ€‘theoretical* (or has an ICSD entry), `False` otherwise.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E01wksoN8MbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your Materials Project API key (or set MP_API_KEY env var)\n",
        "API_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Elements to search for\n",
        "symbols = [\n",
        "    \"Ir\", \"Pt\", \"Ru\", \"Ni\", \"Fe\", \"Mo\",\n",
        "    \"Ta\", \"Nb\", \"Ti\", \"Cu\", \"Sb\", \"Mn\",\n",
        "    \"Co\", \"Sn\"]\n",
        "\n",
        "FIELDS = [\n",
        "    \"material_id\",\n",
        "    \"chemsys\",\n",
        "    \"formula_pretty\",\n",
        "    \"symmetry\",                   # full object â†’ symbol, number, crystal_system, point_group\n",
        "    \"formation_energy_per_atom\",\n",
        "    \"band_gap\",\n",
        "    \"volume\",\n",
        "    \"structure\",                  # for volume fallback\n",
        "    \"elements\",\n",
        "    \"nelements\",\n",
        "    \"density\",\n",
        "    \"energy_above_hull\",\n",
        "]\n",
        "\n",
        "bimetal_docs = []\n",
        "with MPRester(API_KEY) as mpr:\n",
        "    seen = set()\n",
        "    for a in tqdm(symbols, desc=\"outer\"):\n",
        "        seen.add(a)\n",
        "        for b in symbols:\n",
        "            if b in seen:      # skip reversed duplicates\n",
        "                continue\n",
        "            docs = mpr.materials.summary.search(\n",
        "                elements=[a, b],\n",
        "                num_elements=2,\n",
        "                fields=FIELDS,\n",
        "                chunk_size=100,\n",
        "            )\n",
        "            bimetal_docs.extend(docs)\n",
        "\n",
        "print(f\"Fetched {len(bimetal_docs)} bimetallic entries with full fields.\")\n"
      ],
      "metadata": {
        "id": "b0_5PVt3RoQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_CSV = Path(\"bimetal_tidy.csv\")\n",
        "\n",
        "rows = []\n",
        "for doc in bimetal_docs:\n",
        "    sym = getattr(doc, \"symmetry\", None)\n",
        "\n",
        "    space_group     = getattr(sym, \"symbol\", None) if sym else None\n",
        "    sg_number       = getattr(sym, \"number\", None) if sym else None\n",
        "    crystal_system  = getattr(sym, \"crystal_system\", None) if sym else None\n",
        "    point_group     = getattr(sym, \"point_group\", None) if sym else None\n",
        "\n",
        "    vol = getattr(doc, \"volume\", None)\n",
        "    if vol is None and getattr(doc, \"structure\", None):\n",
        "        try:\n",
        "            vol = doc.structure.volume\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"bulk_id\":                    doc.material_id,\n",
        "            \"chemsys\":                    doc.chemsys,\n",
        "            \"formula_pretty\":             doc.formula_pretty,\n",
        "            \"nelements\":                  getattr(doc, \"nelements\", None),\n",
        "            \"elements\":                   getattr(doc, \"elements\", None),\n",
        "            \"band_gap\":                   getattr(doc, \"band_gap\", None),\n",
        "            \"formation_energy_per_atom\":  getattr(doc, \"formation_energy_per_atom\", None),\n",
        "            \"energy_above_hull\":          getattr(doc, \"energy_above_hull\", None),\n",
        "            \"density\":                    getattr(doc, \"density\", None),\n",
        "            \"volume\":                     vol,\n",
        "            \"spacegroup\":                 space_group,\n",
        "            \"sg_number\":                  sg_number,\n",
        "            \"crystal_system\":             crystal_system,\n",
        "            \"point_group\":                point_group,\n",
        "        }\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# stable column order\n",
        "cols = [\n",
        "    \"chemsys\", \"bulk_id\",\"formula_pretty\",\"crystal_system\", \"spacegroup\",\"volume\", \"formation_energy_per_atom\",\n",
        "     \"band_gap\"]\n",
        "\n",
        "df = df[cols].drop_duplicates(\"bulk_id\").reset_index(drop=True)\n",
        "\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"saved â†’ {OUT_CSV.resolve()}\")\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "1hdh1BxOeTEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_experimentally_observed(df: pd.DataFrame, api_key: str, batch=100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add a boolean column 'experimentally_observed' using MP Summary 'theoretical' flag.\n",
        "    Falls back to checking 'icsd_ids' from get_data_by_id when needed.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"experimentally_observed\"] = pd.NA\n",
        "\n",
        "    ids = df[\"bulk_id\"].dropna().astype(str).unique().tolist()\n",
        "    if not ids:\n",
        "        return df\n",
        "\n",
        "    with MPRester(api_key) as mpr:\n",
        "        # ---- primary: use Summary search â†’ theoretical ----\n",
        "        fields = [\"material_id\", \"theoretical\"]\n",
        "        for i in range(0, len(ids), batch):\n",
        "            chunk = ids[i:i+batch]\n",
        "            docs = mpr.materials.summary.search(material_ids=chunk, fields=fields, chunk_size=len(chunk))\n",
        "            for d in docs:\n",
        "                df.loc[df[\"bulk_id\"] == d.material_id, \"experimentally_observed\"] = (not bool(d.theoretical))\n",
        "\n",
        "        # ---- fallback: if anything still NA, check for ICSD entries ----\n",
        "        missing = df[\"experimentally_observed\"].isna()\n",
        "        if missing.any():\n",
        "            for mid in df.loc[missing, \"bulk_id\"].astype(str).unique():\n",
        "                try:\n",
        "                    # use_document_model=False returns plain dict; safer for probing keys\n",
        "                    doc = mpr.materials.summary.get_data_by_id(mid, fields=[\"theoretical\", \"icsd_ids\"],\n",
        "                                                               use_document_model=False)\n",
        "                    # get_data_by_id may return a list or a dict depending on version; normalize\n",
        "                    if isinstance(doc, list) and doc:\n",
        "                        doc = doc[0]\n",
        "                    theoretical = doc.get(\"theoretical\", None)\n",
        "                    icsd_ids = doc.get(\"icsd_ids\", []) or []\n",
        "                    if theoretical is not None:\n",
        "                        val = not bool(theoretical)\n",
        "                    else:\n",
        "                        # if theoretical missing, consider ICSD presence as \"observed\"\n",
        "                        val = bool(icsd_ids)\n",
        "                    df.loc[df[\"bulk_id\"] == mid, \"experimentally_observed\"] = val\n",
        "                except Exception:\n",
        "                    # leave as NA if anything goes wrong\n",
        "                    pass\n",
        "\n",
        "    # cast to boolean where possible\n",
        "    df[\"experimentally_observed\"] = df[\"experimentally_observed\"].astype(\"boolean\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "N8f60yxYX2gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = add_experimentally_observed(df,  API_KEY)"
      ],
      "metadata": {
        "id": "3bQ9cqVLX9Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "7-AFs7e7YKBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step2-label\"></a>Â Stepâ€¯2Â â€“Â LabelÂ â€œInâ€‘Domainâ€â€¯vsâ€¯â€œOutâ€‘ofâ€‘Domainâ€\n"
      ],
      "metadata": {
        "id": "7enZwuFXy7po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"mdâ€‘inâ€‘domain\"></span>1Â Â Tag OC20 structures (in_domain flag)\n"
      ],
      "metadata": {
        "id": "nxh9kVzd-34u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds an in_domain boolean column to our master dataframe df."
      ],
      "metadata": {
        "id": "TyXBCVe---nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# 1. Load the pickled list of dicts\n",
        "with open('/content/bulks.pkl', 'rb') as f:\n",
        "    bulks = pickle.load(f)  # bulks\n",
        "\n",
        "# 2. Extract all src_id values\n",
        "#    We know each entry is a dict with key 'src_id'\n",
        "src_ids = {entry['src_id'] for entry in bulks if 'src_id' in entry}  # list-of-dicts\n",
        "\n",
        "# 3. Flag DataFrame rows whose bulk_id is in that set\n",
        "df['in_domain'] = df['bulk_id'].isin(src_ids)\n",
        "\n",
        "# 4. Quick check\n",
        "print(f\"Found {len(src_ids)} unique src_id values in bulks.pkl\")\n",
        "print(df[['bulk_id', 'in_domain']].head(10))\n"
      ],
      "metadata": {
        "id": "EAtHuzvhz5Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "rVIio91X3L2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"mdâ€‘symâ€‘stats\"></span>2Â Â Crystalâ€‘system / spaceâ€‘group statistics\n",
        "\n",
        "Here we group the dataframe by crystal_system and spacegroup to inspect\n",
        "\n",
        "*   average unitâ€‘cell volume (Meanâ€¯Volume),\n",
        "*   population count (Count), and\n",
        "*   the list of bulk IDs that fall into each symmetry bin.\n",
        "\n"
      ],
      "metadata": {
        "id": "ycV9CA3S_EpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by crystal system and symbol\n",
        "grouped = df.groupby(['crystal_system', 'spacegroup'], sort=False)\n",
        "\n",
        "# Perform aggregations: mean and count of volume, and list out bulk_ids\n",
        "aggregated_data = grouped.agg({\n",
        "    'volume': ['mean', 'count'],\n",
        "    'bulk_id': lambda x: list(x)\n",
        "})\n",
        "\n",
        "# Rename columns for clarity\n",
        "aggregated_data.columns = ['Mean Volume', 'Count', 'Bulk IDs']\n",
        "\n",
        "# Sort by mean volume descending\n",
        "sorted_aggregated_data = aggregated_data.sort_values(by='Mean Volume', ascending=False)\n",
        "\n",
        "# Display the full aggregated table\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(sorted_aggregated_data)\n"
      ],
      "metadata": {
        "id": "wLilUK60eTHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symmetryâ€“volume overview\n",
        "* Rare, low-symmetry lattices (e.g., triclinic **P-1**, monoclinic **C2/c**) show the **largest mean volumes** (> 700 Ã…Â³).  \n",
        "* Common high-symmetry groups (cubic **Fm-3 m**, hexagonal **P6â‚ƒ/mmc**) cluster at **smaller volumes** (â‰ˆ 60â€“135 Ã…Â³) and dominate the dataset."
      ],
      "metadata": {
        "id": "eMXovQeT_-s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['spacegroup'].value_counts().head(10)"
      ],
      "metadata": {
        "id": "b-Lql8qoeTJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['crystal_system'].value_counts().head(5)"
      ],
      "metadata": {
        "id": "DUPvLpHweTME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we filter\n",
        "* Retain the five most frequent crystal systems: **Cubic, Hexagonal, Orthorhombic, Tetragonal, Trigonal**.  \n",
        "* Choose space groups that are (i) well-populated, (ii) span P/I/F centering, and (iii) keep **mean volumes < 135 Ã…Â³**."
      ],
      "metadata": {
        "id": "lXQSF1n2_iPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "selected_spacegroups = [\n",
        "    # â”€â”€ cubic â”€â”€\n",
        "    \"Pm-3m\", \"Fm-3m\", \"Fd-3m\", \"Pm-3n\", \"Im-3m\",\n",
        "    # â”€â”€ orthorhombic â”€â”€\n",
        "    \"Cmmm\", \"Pmmn\", \"Pmma\", \"Fmmm\", \"Immm\", \"Pnnm\",\n",
        "    # â”€â”€ tetragonal â”€â”€\n",
        "    \"I4/mmm\", \"P4/mmm\", \"I4/mcm\",\n",
        "    # â”€â”€ hexagonal â”€â”€\n",
        "    \"P6_3/mmc\",\n",
        "    \"P-6m2\",\n",
        "    # â”€â”€ trigonal â”€â”€\n",
        "    \"R-3m\",\n",
        "]\n",
        "\n",
        "df_selected = df[\n",
        "    df[\"spacegroup\"].isin(selected_spacegroups)\n",
        "].copy()\n",
        "\n",
        "df_selected.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "display(df_selected.head())"
      ],
      "metadata": {
        "id": "BFF-aBqxeTO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EG1PnZwM_qVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_in = df_selected[df_selected[\"in_domain\"]].copy()"
      ],
      "metadata": {
        "id": "dvwqGvyNeTSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in['band_gap'].value_counts()"
      ],
      "metadata": {
        "id": "q1Pe8DMeeTXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in['formation_energy_per_atom'].hist()"
      ],
      "metadata": {
        "id": "KoRUGGgB4bW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"mdâ€‘saveâ€‘subset\"></span>4Â Â Save OC20â€‘matching structures (in_domain.pkl)\n",
        "This final cell pulls the actual bulk structures (ASE Atoms objects) that\n",
        "correspond to the IDs in df_in, then saves the subset as in_domain.pkl"
      ],
      "metadata": {
        "id": "0QUAhdYoCLxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- paths ---\n",
        "OUT_PKL   = \"/content/in_domain.pkl\"   # output\n",
        "\n",
        "# --- 1) get the whitelist of IDs from df_in ---\n",
        "assert \"bulk_id\" in df_in.columns, \"df_in must have a 'bulk_id' column\"\n",
        "allowed_ids = set(df_in[\"bulk_id\"].astype(str).unique())\n",
        "\n",
        "\n",
        "# --- 3) filter by src_id âˆˆ allowed_ids ---\n",
        "in_domain = [entry for entry in bulks\n",
        "             if isinstance(entry, dict) and str(entry.get(\"src_id\")) in allowed_ids]\n",
        "\n",
        "# --- 4) save subset ---\n",
        "with open(OUT_PKL, \"wb\") as f:\n",
        "    pickle.dump(in_domain, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"bulks total:   {len(bulks)}\")\n",
        "print(f\"unique IDs in df_in: {len(allowed_ids)}\")\n",
        "print(f\"kept (in-domain): {len(in_domain)}\")\n",
        "print(f\"saved â†’ {OUT_PKL}\")\n"
      ],
      "metadata": {
        "id": "Mm-OOHcteTT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OUT-OF-DOMAIN Bulk Data Set Analysis"
      ],
      "metadata": {
        "id": "XHiU5xGZJB3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = df_selected[~df_selected[\"in_domain\"]].copy()"
      ],
      "metadata": {
        "id": "Z66bMPae4bcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step3-enrich\"></a>Â Â EnrichÂ MissingÂ SymmetryÂ &Â Properties from OC20 pickle file\n",
        "\n",
        "The initial **OC20** `bulks.pkl` catalogue occasionally contains entries whose Materials Project (MP) records are incomplete or stale.  \n",
        "To create a **robust and upâ€‘toâ€‘date outâ€‘ofâ€‘domain dataset**, we therefore run a twoâ€“pronged enrichment step:\n",
        "\n",
        "1. **Local symmetry prediction** for structures lacking crystallographic metadata.  \n",
        "2. **Compositionâ€‘based data filling** for any residual thermodynamicâ€¯/â€¯electronic gaps.\n",
        "\n",
        "The code cell below implements the complete workflow.\n",
        "\n",
        "---\n",
        "\n",
        "#### <a name=\"subsec-symmetry-pred\"></a>Â PredictÂ SymmetryÂ Locally\n",
        "Crystallographic symmetry is pivotal for classifying and rationalising materials.  \n",
        "For any entry missing *spaceâ€¯group* or *crystalâ€¯system*, we call **`predict_symmetry`**, which:\n",
        "\n",
        "* converts the ASE `Atoms` object to a `pymatgen` **`Structure`**,  \n",
        "* invokes `SpacegroupAnalyzer` at a tolerance of `symprecâ€¯=â€¯5â€¯Ã—â€¯10â»Â³`,  \n",
        "* returns the inferred `crystal_system`, `space_group`, and `point_group`.\n",
        "\n",
        "This guarantees a consistent structural description across the full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### <a name=\"subsec-gap-fill\"></a>Â FillÂ NumericÂ GapsÂ byÂ CompositionÂ Match\n",
        "Key thermodynamic and electronic quantitiesâ€”**formation energy**, **bandâ€¯gap**, **volume**â€”may still be absent for certain `bulk_id`s.  \n",
        "The helper **`fill_missing_props`** addresses this as follows:\n",
        "\n",
        "1. Query MP for **all polymorphs** sharing the *exact chemical formula* and *identical symmetry* (space group or point group).  \n",
        "2. Rank candidates by **`energy_above_hull`** and choose the *most stable* structure.  \n",
        "3. Transfer its formation energy, band gap, and volume to the target entry.  \n",
        "4. Propagate the `theoretical` flag so that `experimentally_observed` is updated consistently.\n",
        "\n",
        "This chemically intuitive heuristic ensures every composition is represented by the **thermodynamically most favourable** data available.\n"
      ],
      "metadata": {
        "id": "J_BMCE5XFAIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0) helpers ---------------------------------------------------------------\n",
        "\n",
        "FIELDS_MAIN = [\n",
        "    \"material_id\",\n",
        "    \"formation_energy_per_atom\", \"band_gap\", \"volume\",\n",
        "    \"symmetry.crystal_system\", \"symmetry.symbol\", \"symmetry.point_group\",\n",
        "    \"theoretical\",                  # â† will drive experimentally_observed\n",
        "]\n",
        "\n",
        "COL_MAP = {\n",
        "    \"formation_energy_per_atom\": \"formation_energy_per_atom\",\n",
        "    \"band_gap\":                   \"band_gap\",\n",
        "    \"volume\":                     \"volume\",\n",
        "    \"symmetry.crystal_system\":    \"crystal_system\",\n",
        "    \"symmetry.symbol\":            \"space_group\",\n",
        "    \"symmetry.point_group\":       \"point_group\",\n",
        "    \"theoretical\":                \"theoretical\",\n",
        "}\n",
        "\n",
        "def set_observed_from_theoretical(df):\n",
        "    \"\"\"Create/refresh 'experimentally_observed' from 'theoretical'.\"\"\"\n",
        "    if \"experimentally_observed\" not in df.columns:\n",
        "        df[\"experimentally_observed\"] = None\n",
        "    mask = df[\"theoretical\"].notna()\n",
        "    df.loc[mask, \"experimentally_observed\"] = ~df.loc[mask, \"theoretical\"].astype(bool)\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- 1) build df_filtered (unchanged) ----------------------------------------\n",
        "\n",
        "symbols = [\"Ir\",\"Pt\",\"Ru\",\"Ni\",\"Co\",\"Fe\",\"Mn\",\"Ta\",\"Ti\",\"Nb\",\"Mo\",\"Cu\",\"Sn\",\"Sb\"]\n",
        "valid_pairs = {tuple(sorted(p)) for p in combinations(symbols, 2)}\n",
        "\n",
        "filtered = []\n",
        "for entry in bulks:\n",
        "    atoms = entry.get(\"atoms\")\n",
        "    if isinstance(atoms, Atoms):\n",
        "        elems = sorted(set(atoms.get_chemical_symbols()))\n",
        "        if len(elems) == 2 and tuple(elems) in valid_pairs:\n",
        "            filtered.append({\n",
        "                \"chemsys\": f\"{elems[0]}-{elems[1]}\",\n",
        "                \"bulk_id\": entry[\"src_id\"],\n",
        "                \"formula\": atoms.get_chemical_formula(),\n",
        "                \"atoms\":   atoms,\n",
        "            })\n",
        "\n",
        "df_filtered = pd.DataFrame(filtered)\n",
        "\n",
        "# pre-create columns we will fill\n",
        "for out_col in COL_MAP.values():\n",
        "    if out_col not in df_filtered.columns:\n",
        "        df_filtered[out_col] = None\n",
        "\n",
        "\n",
        "# --- 2) fetch properties in batches (includes 'theoretical') -----------------\n",
        "\n",
        "def fetch_props_for_ids(df, api_key, batch_size=40):\n",
        "    with MPRester(api_key) as mpr:\n",
        "        ids = df[\"bulk_id\"].tolist()\n",
        "        for i in range(0, len(ids), batch_size):\n",
        "            batch = ids[i:i+batch_size]\n",
        "            try:\n",
        "                docs = mpr.materials.summary.search(\n",
        "                    material_ids=batch,\n",
        "                    fields=FIELDS_MAIN,\n",
        "                    chunk_size=len(batch)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                # retry one-by-one if a server timeout happens\n",
        "                for mid in batch:\n",
        "                    try:\n",
        "                        dlist = mpr.materials.summary.search(\n",
        "                            material_ids=[mid],\n",
        "                            fields=FIELDS_MAIN,\n",
        "                            chunk_size=1\n",
        "                        )\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    docs.extend(dlist)\n",
        "\n",
        "            for doc in docs:\n",
        "                idx = df.index[df[\"bulk_id\"] == doc.material_id]\n",
        "                if len(idx) == 0:\n",
        "                    continue\n",
        "                j = idx[0]\n",
        "\n",
        "                for api_f, df_f in COL_MAP.items():\n",
        "                    if \".\" in api_f:\n",
        "                        p0, p1 = api_f.split(\".\")\n",
        "                        parent = getattr(doc, p0, None)\n",
        "                        val = getattr(parent, p1, None) if parent is not None else None\n",
        "                    else:\n",
        "                        val = getattr(doc, api_f, None)\n",
        "\n",
        "                    if val is not None:\n",
        "                        df.at[j, df_f] = val\n",
        "\n",
        "    # derive 'experimentally_observed'\n",
        "    return set_observed_from_theoretical(df)\n",
        "\n",
        "df_filtered = fetch_props_for_ids(df_filtered, API_KEY)\n",
        "\n",
        "\n",
        "# --- 3) predict symmetry where missing (unchanged) ---------------------------\n",
        "\n",
        "def predict_symmetry(atoms):\n",
        "    struct = AseAtomsAdaptor.get_structure(atoms)\n",
        "    sga = SpacegroupAnalyzer(struct, symprec=5e-3, angle_tolerance=5.0)\n",
        "    return {\n",
        "        \"crystal_system\": sga.get_crystal_system().capitalize(),\n",
        "        \"space_group\":    sga.get_space_group_symbol(),\n",
        "        \"point_group\":    sga.get_point_group_symbol(),\n",
        "    }\n",
        "\n",
        "need_sym = df_filtered[\"space_group\"].isna() | df_filtered[\"point_group\"].isna()\n",
        "for idx in df_filtered[need_sym].index:\n",
        "    atoms = df_filtered.at[idx, \"atoms\"]\n",
        "    if atoms is None:\n",
        "        continue\n",
        "    try:\n",
        "        sym = predict_symmetry(atoms)\n",
        "        for k, v in sym.items():\n",
        "            df_filtered.at[idx, k] = v\n",
        "    except Exception as e:\n",
        "        print(f\"Symmetry prediction failed for {df_filtered.at[idx, 'bulk_id']}: {e}\")\n",
        "\n",
        "\n",
        "# --- 4) fill remaining numeric props by composition match --------------------\n",
        "\n",
        "from pymatgen.core import Composition\n",
        "\n",
        "def fill_missing_props(df, api_key):\n",
        "    TARGET = [\"formation_energy_per_atom\", \"band_gap\", \"volume\"]\n",
        "    df[TARGET] = df[TARGET].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    miss = df[TARGET].isna().any(axis=1)\n",
        "    if not miss.any():\n",
        "        return set_observed_from_theoretical(df)   # ensure observed set\n",
        "\n",
        "    with MPRester(api_key) as mpr:\n",
        "        for idx, row in df[miss].iterrows():\n",
        "            formula = row.get(\"formula\")\n",
        "            if not isinstance(formula, str) or pd.isna(formula):\n",
        "                continue\n",
        "            sym_ref = row.get(\"space_group\") or row.get(\"point_group\")\n",
        "            if not sym_ref:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                comp = Composition(formula)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            docs = mpr.materials.summary.search(\n",
        "                formula=formula,\n",
        "                num_elements=len(comp.elements),\n",
        "                fields=TARGET + [\"symmetry\", \"energy_above_hull\", \"composition\", \"theoretical\"],\n",
        "                chunk_size=300,\n",
        "            )\n",
        "\n",
        "            best = None\n",
        "            best_eh = float(\"inf\")\n",
        "            for d in docs:\n",
        "                if d.composition is None or d.symmetry is None:\n",
        "                    continue\n",
        "                try:\n",
        "                    if Composition(d.composition) != comp:\n",
        "                        continue\n",
        "                except Exception:\n",
        "                    continue\n",
        "                if d.symmetry.symbol == sym_ref or d.symmetry.point_group == sym_ref:\n",
        "                    eh = d.energy_above_hull if d.energy_above_hull is not None else float(\"inf\")\n",
        "                    if eh < best_eh:\n",
        "                        best, best_eh = d, eh\n",
        "\n",
        "            if best is not None:\n",
        "                df.at[idx, \"formation_energy_per_atom\"] = best.formation_energy_per_atom\n",
        "                df.at[idx, \"band_gap\"] = best.band_gap\n",
        "                df.at[idx, \"volume\"] = best.volume\n",
        "                # also set theoretical â†’ observed if we didnâ€™t have it\n",
        "                if pd.isna(df.at[idx, \"theoretical\"]) and getattr(best, \"theoretical\", None) is not None:\n",
        "                    df.at[idx, \"theoretical\"] = bool(best.theoretical)\n",
        "\n",
        "    # refresh observed column\n",
        "    return set_observed_from_theoretical(df)\n",
        "\n",
        "df_filtered = fill_missing_props(df_filtered, API_KEY)\n",
        "\n",
        "print(\"Done. df_filtered shape:\", df_filtered.shape)\n"
      ],
      "metadata": {
        "id": "Nr-HyO4V4bfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.columns"
      ],
      "metadata": {
        "id": "aL77Kjwr4biD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overwrite df_filtered with tidy columns (atoms last)\n",
        "df_filtered = (\n",
        "    df_filtered.rename(columns={\"formula\": \"formula_pretty\", \"space_group\": \"spacegroup\"})\n",
        ")\n",
        "\n",
        "# add any missing columns with NA\n",
        "for col in [\"density\", \"energy_above_hull\", \"in_domain\", \"atoms\"]:\n",
        "    if col not in df_filtered.columns:\n",
        "        df_filtered[col] = pd.NA\n",
        "\n",
        "# drop columns you don't want\n",
        "if \"point_group\" in df_filtered.columns:\n",
        "    df_filtered = df_filtered.drop(columns=[\"point_group\"])\n",
        "\n",
        "# final order\n",
        "order = [\n",
        "    \"chemsys\",\n",
        "    \"bulk_id\",\n",
        "    \"formula_pretty\",\n",
        "    \"crystal_system\",\n",
        "    \"spacegroup\",\n",
        "    \"volume\",\n",
        "    \"formation_energy_per_atom\",\n",
        "    \"band_gap\",\n",
        "    \"experimentally_observed\",\n",
        "    \"atoms\"]\n",
        "df_filtered = df_filtered.reindex(columns=order)\n"
      ],
      "metadata": {
        "id": "xOz8_mKeDfFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered"
      ],
      "metadata": {
        "id": "onjfZrVXDMoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_spacegroups = [\n",
        "    # cubic\n",
        "    \"Pm-3m\", \"Fm-3m\", \"Fd-3m\", \"Pm-3n\", \"Im-3m\",\n",
        "    # orthorhombic\n",
        "    \"Cmmm\", \"Pmmn\", \"Pmma\", \"Fmmm\", \"Immm\", \"Pnnm\",\n",
        "    # tetragonal\n",
        "    \"I4/mmm\", \"P4/mmm\", \"I4/mcm\",\n",
        "    # hexagonal\n",
        "    \"P6_3/mmc\",\n",
        "    \"P-6m2\",\n",
        "    # trigonal\n",
        "    \"R-3m\",\n",
        "]\n",
        "\n",
        "# 1)  all matching rows\n",
        "df_filtered_selected = df_filtered[\n",
        "    df_filtered[\"spacegroup\"].isin(selected_spacegroups)\n",
        "].copy()\n",
        "\n",
        "# Preview\n",
        "display(df_filtered_selected.head())"
      ],
      "metadata": {
        "id": "TS5uigzYbvnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify new entries by bulk_id\n",
        "new_entries = df_filtered_selected[~df_filtered_selected['bulk_id'].isin(df_in['bulk_id'])]\n",
        "\n",
        "# Step 2: Prepare new entries for merging\n",
        "# Add missing columns and set in_domain to False\n",
        "new_entries = new_entries.assign(in_domain=False)\n",
        "\n",
        "# Step 3: Align columns with df_out\n",
        "# Select only the columns present in df_out\n",
        "df_out_columns = df_out.columns.tolist()\n",
        "new_entries = new_entries[df_out_columns]\n",
        "\n",
        "# Step 4: Merge with df_out\n",
        "df_out_updated = pd.concat([df_out, new_entries], ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Added {len(new_entries)} new entries to the dataset\")\n",
        "print(f\"Updated dataset now has {len(df_out_updated)} entries\")"
      ],
      "metadata": {
        "id": "rY2hfxcOKsxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure that our outâ€‘ofâ€‘domain dataset remains consistent with the objectives of this study, we remove a small number of Materials Project entries that do not meet our bulkâ€‘screening criteria:\n",
        "\n",
        "- **mpâ€‘1213433**  \n",
        "  This entry corresponds to a twoâ€‘dimensional Cuâ€“Pt layered structure rather than a true 3D bulk phase. Such quasiâ€‘2D configurations fall outside the scope of our bulk stability analysis and are thus excluded.  \n",
        "\n",
        "- **mpâ€‘1208216**, **mpâ€‘1537768**: These compositions were uploaded to the Materials Project after our initial screening cutoff and have not been characterized within our target dataset. To maintain consistency with our articleâ€™s data format and avoid introducing newly added, unvalidated structures, we therefore exclude them from df_out_updated.\n",
        "\n",
        "\n",
        "The following code cell applies this exclusion filter:"
      ],
      "metadata": {
        "id": "VKNGzwDCVOmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set of MPID objects to drop\n",
        "remove_set = {\"mp-1208216\", \"mp-1213433\", \"mp-1537768\"}\n",
        "\n",
        "df_out_updated = df_out_updated[~df_out_updated['bulk_id'].isin(remove_set)]"
      ],
      "metadata": {
        "id": "XSpPXvQcK6J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "R24dRAUULoHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MPRester with your API key\n",
        "\n",
        "mpr = MPRester(API_KEY)\n",
        "\n",
        "# Create a new column for atomic structures\n",
        "df_out_updated['atoms'] = None\n",
        "\n",
        "# Get unique bulk IDs that need atomic structures\n",
        "bulk_ids = df_out_updated['bulk_id'].unique().tolist()\n",
        "\n",
        "# Fetch structures in batches\n",
        "batch_size = 1000\n",
        "for i in range(0, len(bulk_ids), batch_size):\n",
        "    batch = bulk_ids[i:i+batch_size]\n",
        "\n",
        "    # Fetch material data with structures\n",
        "    docs = mpr.materials.summary.search(\n",
        "        material_ids=batch,\n",
        "        fields=[\"material_id\", \"structure\"]\n",
        "    )\n",
        "\n",
        "    # Create mapping from material ID to structure\n",
        "    structure_map = {doc.material_id: doc.structure for doc in docs}\n",
        "\n",
        "    # Convert structures to ASE Atoms and assign to dataframe\n",
        "    for idx, row in df_out_updated.iterrows():\n",
        "        if row['bulk_id'] in structure_map:\n",
        "            struct = structure_map[row['bulk_id']]\n",
        "            atoms = AseAtomsAdaptor.get_atoms(struct)\n",
        "            df_out_updated.at[idx, 'atoms'] = atoms"
      ],
      "metadata": {
        "id": "p9c12ig1LwcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create mapping dictionary from df_filtered_selected\n",
        "atoms_mapping = {}\n",
        "for _, row in df_filtered_selected.iterrows():\n",
        "    bulk_id = row['bulk_id']\n",
        "    atoms = row['atoms']\n",
        "    if not pd.isna(atoms) and atoms is not None:  # Ensure valid atoms object\n",
        "        atoms_mapping[bulk_id] = atoms\n",
        "\n",
        "# Step 2: Fill atoms from mapping dictionary\n",
        "def fill_atoms(row):\n",
        "    if (pd.isna(row['atoms']) or (row['atoms'] is None)):\n",
        "        return atoms_mapping.get(row['bulk_id'], None)\n",
        "    return row['atoms']\n",
        "\n",
        "df_out_updated['atoms'] = df_out_updated.apply(fill_atoms, axis=1)"
      ],
      "metadata": {
        "id": "NOWboZhHMaqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated.info()"
      ],
      "metadata": {
        "id": "nAjw5DCDMjUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "KTvV6llUMltl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in['experimentally_observed'].value_counts()"
      ],
      "metadata": {
        "id": "pHKbsypkXvPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_in"
      ],
      "metadata": {
        "id": "l6QtNe_deDwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated.columns"
      ],
      "metadata": {
        "id": "PoJxEYsoeUV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step4-viz\"></a>Â Stepâ€¯4Â â€“Â ExploratoryÂ Visualisation\n"
      ],
      "metadata": {
        "id": "WnlzeSt4Kvjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import NullLocator\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "YCOL = \"formation_energy_per_atom\"\n",
        "XCAT = \"crystal_system\"\n",
        "HUE  = \"experimentally_observed\"      # boolean column\n",
        "\n",
        "DECOMP_ENERGY   = 0.2\n",
        "KINETIC_BARRIER = 0.5\n",
        "\n",
        "COL_OBS  = \"#08306B\"  # dark blue\n",
        "COL_NOBS = \"#67000D\"  # dark red\n",
        "EDGE     = \"black\"\n",
        "\n",
        "def _color_violins(vdict, face, edge=EDGE, alpha=0.85, lw=0.8):\n",
        "    for b in vdict[\"bodies\"]:\n",
        "        b.set_facecolor(face)\n",
        "        b.set_edgecolor(edge)\n",
        "        b.set_alpha(alpha)\n",
        "        b.set_linewidth(lw)\n",
        "\n",
        "def _panel(ax, df, ylim):\n",
        "    cats = sorted(df[XCAT].astype(str).unique())\n",
        "    pos  = np.arange(1, len(cats) + 1, dtype=float)\n",
        "    off  = 0.12\n",
        "\n",
        "    d_false = [df[(df[XCAT].astype(str) == c) & (~df[HUE])][YCOL].values for c in cats]\n",
        "    d_true  = [df[(df[XCAT].astype(str) == c) & ( df[HUE])][YCOL].values for c in cats]\n",
        "\n",
        "    v_false = ax.violinplot(d_false, positions=pos - off, widths=0.22,\n",
        "                            showmeans=False, showextrema=False, showmedians=False)\n",
        "    v_true  = ax.violinplot(d_true,  positions=pos + off, widths=0.22,\n",
        "                            showmeans=False, showextrema=False, showmedians=False)\n",
        "\n",
        "    _color_violins(v_false, COL_NOBS)\n",
        "    _color_violins(v_true,  COL_OBS)\n",
        "\n",
        "    ax.axhline(DECOMP_ENERGY,   linestyle=\"--\", linewidth=1.2, alpha=0.9)\n",
        "    ax.axhline(KINETIC_BARRIER, linestyle=\"--\", linewidth=1.2, alpha=0.9)\n",
        "\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.set_xlim(0.5, len(cats) + 0.5)\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.25, zorder=0)\n",
        "    ax.set_xlabel(\"Crystal System\")\n",
        "    ax.set_ylabel(\"Formation Energy (eV/atom)\")\n",
        "    ax.xaxis.set_minor_locator(NullLocator())\n",
        "    ax.yaxis.set_minor_locator(NullLocator())\n",
        "    ax.tick_params(axis=\"x\", which=\"major\", direction=\"out\", length=8, width=2, rotation=45)\n",
        "    ax.tick_params(axis=\"y\", which=\"major\", direction=\"out\", length=8, width=2)\n",
        "    for sp in (\"bottom\", \"left\"):\n",
        "        ax.spines[sp].set_linewidth(2)\n",
        "\n",
        "    # â‰¤5 evenly spaced tick labels\n",
        "    if len(cats) <= 5:\n",
        "        tick_pos, tick_lab = pos, cats\n",
        "    else:\n",
        "        idx = sorted({int(round(i)) for i in np.linspace(0, len(cats) - 1, 5)})[:5]\n",
        "        tick_pos = pos[idx]\n",
        "        tick_lab = [cats[i] for i in idx]\n",
        "    ax.set_xticks(tick_pos, tick_lab)\n",
        "    for lab in ax.get_xticklabels():\n",
        "        lab.set_ha(\"right\")\n",
        "\n",
        "# ---- Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "_panel(ax1, df_in,           ylim=(-1.25, 0.60))\n",
        "_panel(ax2, df_out_updated,  ylim=(-1.25, 1.25))\n",
        "\n",
        "ax1.set_title(\"In Domain Structures\")\n",
        "ax2.set_title(\"Out of Domain Structures\")\n",
        "\n",
        "# --- Annotations on left panel\n",
        "ax1.text(0.98, KINETIC_BARRIER, \"Kinetically\\nAccessible\",\n",
        "         fontsize=14, fontweight=\"bold\", ha=\"left\", va=\"center\",\n",
        "         transform=ax1.get_yaxis_transform(which=\"grid\"))\n",
        "ax1.text(0.98, DECOMP_ENERGY, \"Thermodynamically\\nStable\",\n",
        "         fontsize=14, fontweight=\"bold\", ha=\"left\", va=\"center\",\n",
        "         transform=ax1.get_yaxis_transform(which=\"grid\"))\n",
        "\n",
        "# shared legend\n",
        "handles = [Patch(facecolor=COL_OBS,  edgecolor=EDGE, label=\"Observed\"),\n",
        "           Patch(facecolor=COL_NOBS, edgecolor=EDGE, label=\"Not Observed\")]\n",
        "fig.legend(handles=handles, loc=\"upper center\", ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "base = \"/content/formation_energy_stability\"\n",
        "for ext in (\"pdf\", \"png\", \"tiff\"):\n",
        "    plt.savefig(f\"{base}.{ext}\", dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z-h2h1h8eVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated['bulk_id'].nunique()"
      ],
      "metadata": {
        "id": "xbY3lLDfijLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated"
      ],
      "metadata": {
        "id": "CUgWHrrYmRHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step5-vasp\"></a>Â Stepâ€¯5Â â€“Â GenerateÂ VASPÂ InputÂ Decks\n"
      ],
      "metadata": {
        "id": "I0-yCZ5VK0LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> âš ï¸ **VASP license required**  \n",
        "> The next cell writes full VASP input folders from each `ase.Atoms` object.  \n",
        "> You must be an **authorized VASP user** with access to PAW-PBE POTCARs; otherwise the cell will fail and you may violate the VASP EULA.\n",
        "\n",
        "| File        | Purpose (edit paths marked **EDIT HERE**) |\n",
        "|-------------|-------------------------------------------|\n",
        "| `POSCAR`    | Bulk geometry from `ase.Atoms`. |\n",
        "| `KPOINTS`   | Fixed Î“-centered 10 Ã— 10 Ã— 10 mesh. |\n",
        "| `INCAR`     | Generic bulkâ€relax settings (`BULK_VASP_FLAGS`). |\n",
        "| `POTCAR`    | Concatenated from `<POTCAR_DIR>/<elem>/POTCAR`. |\n",
        "| `job.sh`    | Example SLURM script â€” adapt queue, time, modules. |\n",
        "\n",
        "**Customise before running**\n",
        "\n",
        "1. **`BASE_DIR`** â€“ where run folders are created *(EDIT HERE)*.  \n",
        "2. **`POTCAR_DIR`** â€“ root of your pseudopotential library *(EDIT HERE)*.  \n",
        "3. `write_batch_script()` â€“ tweak `#SBATCH` lines for your HPC.  \n",
        "\n",
        "The helper routines are cluster-agnostic; only path strings and the SLURM header are site-specific.\n",
        "\n",
        "After relaxation, results are uploaded to Google Drive; the notebook then parses each `OUTCAR`, converts the relaxed structures back to pickled `ase.Atoms`, and feeds them into the downstream MLIP relaxation sub-routine.  \n",
        "A full end-to-end demonstration is available in the companion tutorial:  \n",
        "<https://github.com/ergroup/EqV2-HER-Discovery/blob/main/notebooks/demo_InDomain_H_ads.ipynb>\n"
      ],
      "metadata": {
        "id": "4CpmtnzHNu4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 1) VASP flags for OC20 bulk (check the github repository https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/utils/vasp.py)\n",
        "# ----------------------------------------------------------------------------------\n",
        "BULK_VASP_FLAGS = {\n",
        "    \"ibrion\": 1,         # ionic optimisation (quasiâ€‘Newton)\n",
        "    \"nsw\": 100,          # max ionic steps\n",
        "    \"isif\": 7,           # relax both ions and cell\n",
        "    \"isym\": 0,           # symmetry off (safer for lowâ€‘symmetry structures)\n",
        "    \"ediffg\": 1e-8,      # forceâ€‘convergence criterion\n",
        "    \"encut\": 500.0,      # planeâ€‘wave cutoff (eV)\n",
        "    \"kpts\": (10, 10, 10),\n",
        "    \"prec\": \"Accurate\",\n",
        "    \"gga\": \"RP\",\n",
        "    \"pp\":  \"PBE\",\n",
        "    \"lwave\":  False,\n",
        "    \"lcharg\": False,\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 2) Fileâ€‘writer helpers\n",
        "# ----------------------------------------------------------------------------------\n",
        "def write_poscar(atoms: Atoms, folder: str):\n",
        "    \"\"\"POSCAR from ASE Atoms.\"\"\"\n",
        "    write(Path(folder, \"POSCAR\"), atoms, format=\"vasp\")\n",
        "\n",
        "def write_kpoints(folder: str):\n",
        "    k0, k1, k2 = BULK_VASP_FLAGS[\"kpts\"]\n",
        "    Path(folder, \"KPOINTS\").write_text(\n",
        "        f\"Automatic mesh\\n0\\nMonkhorstâ€“Pack\\n{k0} {k1} {k2}\\n0 0 0\\n\"\n",
        "    )\n",
        "\n",
        "def create_potcar(folder: str, potcar_root: str):\n",
        "    \"\"\"\n",
        "    Concatenate POTCAR fragments â€“ requires\n",
        "    `<potcar_root>/<Element>/POTCAR` for every species in POSCAR.\n",
        "    \"\"\"\n",
        "    elems = Path(folder, \"POSCAR\").read_text().splitlines()[5].split()\n",
        "    txt = []\n",
        "    for Z in elems:\n",
        "        psrc = Path(potcar_root, Z, \"POTCAR\")\n",
        "        if not psrc.is_file():\n",
        "            raise FileNotFoundError(f\"No POTCAR for {Z} â†’ {psrc}\")\n",
        "        txt.append(psrc.read_text())\n",
        "    Path(folder, \"POTCAR\").write_text(\"\".join(txt))\n",
        "\n",
        "def write_incar(folder: str):\n",
        "    lines = []\n",
        "    for k, v in BULK_VASP_FLAGS.items():\n",
        "        v_fmt = \".TRUE.\" if v is True else \".FALSE.\" if v is False else v\n",
        "        lines.append(f\"{k.upper()} = {v_fmt}\")\n",
        "    Path(folder, \"INCAR\").write_text(\"\\n\".join(lines))\n",
        "\n",
        "def write_batch_script(folder: str):\n",
        "    \"\"\"\n",
        "    Example SLURM launcher  â€“ **edit for your HPC environment**.\n",
        "    \"\"\"\n",
        "    job = Path(folder).name\n",
        "    Path(folder, \"job.sh\").write_text(f\"\"\"#!/bin/bash\n",
        "#SBATCH -N xx\n",
        "#SBATCH -n xx\n",
        "#SBATCH --time=xx:00:00       # <<< EDIT HERE\n",
        "#SBATCH -p xxx              # <<< EDIT HERE\n",
        "#SBATCH -J {job}\n",
        "#SBATCH --output=out.%j\n",
        "#SBATCH --error=err.%j\n",
        "\n",
        "module load xx             # <<< EDIT HERE\n",
        "module load VASP5/5.4.4.xx  # <<< EDIT HERE\n",
        "srun vasp_std > vasp.out\n",
        "\"\"\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 3) Driver â€“ iterate over DataFrame rows\n",
        "# ----------------------------------------------------------------------------------\n",
        "def generate_vasp_inputs(df, base_dir, potcar_dir):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Must contain columns 'bulk_id' and 'atoms'.\n",
        "    base_dir : str or Path\n",
        "        Parent directory where <bulk_id>/ folders will be created.\n",
        "    potcar_dir : str or Path\n",
        "        Root folder containing elementâ€‘wise POTCAR subâ€‘dirs.\n",
        "    \"\"\"\n",
        "    base_dir  = Path(base_dir).expanduser().resolve()\n",
        "    potcar_dir = Path(potcar_dir).expanduser().resolve()\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        bid   = row[\"bulk_id\"]\n",
        "        atoms = row[\"atoms\"]\n",
        "        work  = base_dir / bid\n",
        "        work.mkdir(exist_ok=True)\n",
        "\n",
        "        write_poscar(atoms, work)\n",
        "        write_kpoints(work)\n",
        "        create_potcar(work, potcar_dir)\n",
        "        write_incar(work)\n",
        "        write_batch_script(work)\n",
        "\n",
        "        print(f\"âœ”  VASP inputs written â†’ {work}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 4) Example call  (uncomment & adapt paths)\n",
        "# ----------------------------------------------------------------------------------\n",
        "# BASE_DIR   = \"/path/to/vasp_runs/out_of_domain\"   # <<< EDIT HERE\n",
        "# POTCAR_DIR = \"/path/to/pseudopotentials\"          # <<< EDIT HERE\n",
        "# generate_vasp_inputs(df_out_updated, BASE_DIR, POTCAR_DIR)"
      ],
      "metadata": {
        "id": "ODhibkWeOhgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-outcar-extract\"></a>Stepâ€¯6Â â€“Â Build the **Outâ€‘ofâ€‘Domain** Bulk Pickle from VASP *OUTCAR*s  \n",
        "This cell harvests **fullyâ€‘relaxed bulk geometries and their singleâ€‘point energies** from VASP calculations that were executed for the outâ€‘ofâ€‘domain candidates identified earlier.\n",
        "\n",
        "**Workflow**\n",
        "\n",
        "1. **User parameters**  \n",
        "   * `base_directory` â€“ root folder that contains one subâ€‘directory per material (`mpâ€‘*`), each holding an `OUTCAR`.  \n",
        "   * `output_pickle`Â  â€“ target path for the consolidated pickle (`OutOfDomainBulk.pkl`).  \n",
        "   * `sampling_template` â€“ optional string for attaching an arbitrary provenance tag (`bulk_sampling_str`).\n",
        "\n",
        "2. **Iterate through `mpâ€‘*/OUTCAR` files**  \n",
        "   * Skip folders without a valid `OUTCAR`.  \n",
        "   * Read the **final frame** (`indexÂ =Â -1`) with `ase.io.read`, which attaches a `SinglePointCalculator` containing the converged total energy, forces, and stress.\n",
        "\n",
        "3. **Integrity checks**  \n",
        "   * Verify that the attached calculator is indeed an `ase.calculators.singlepoint.SinglePointCalculator`.  \n",
        "   * Emit warnings for missing or malformed outputs without interrupting the loop.\n",
        "\n",
        "4. **Collect metadata**  \n",
        "   * Store `src_id` (folder name), a random `bulk_sampling_str` (for traceability), and the full `Atoms` object (including the calculator).\n",
        "\n",
        "5. **Serialise**  \n",
        "   * Dump the accumulated list to `output_pickle` with `pickle.HIGHEST_PROTOCOL`.\n",
        "\n",
        "**Outcome**  \n",
        "A single fileÂ â€”Â `OutOfDomainBulk.pkl`Â â€”Â that bundles every outâ€‘ofâ€‘domain bulk structure together with its DFT total energy, ready for downstream modelling or database ingestion.\n",
        "\n"
      ],
      "metadata": {
        "id": "zi4UWyVQK6GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€ User parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "base_directory = \"/content/drive/MyDrive/OutOfDomain\"   # folder containing mp-*/OUTCAR\n",
        "output_pickle  = \"/content/OutOfDomainBulk.pkl\"\n",
        "sampling_template = \"{run}/{step}_{run2}/{step2}\"       # sampling\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "atoms_list = []\n",
        "\n",
        "for subdir in sorted(os.listdir(base_directory)):\n",
        "    if not subdir.startswith(\"mp-\"):\n",
        "        continue\n",
        "\n",
        "    outcar_path = os.path.join(base_directory, subdir, \"OUTCAR\")\n",
        "    if not os.path.isfile(outcar_path):\n",
        "        print(f\"âš ï¸  OUTCAR missing for {subdir}, skipping\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # read final frame with attached SinglePointCalculator\n",
        "        atoms = read(outcar_path, format=\"vasp-out\", index=-1)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to read {outcar_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Sanity check: calculator should now be a SinglePointCalculator\n",
        "    calc = atoms.calc\n",
        "    if not isinstance(calc, SinglePointCalculator):\n",
        "        print(f\"âš ï¸  {subdir}: got calc={calc!r} (not SinglePointCalculator)\")\n",
        "\n",
        "    # build your sampling string however you like\n",
        "    sampling_str = sampling_template.format(\n",
        "        run    = random.randint(0,999),\n",
        "        step   = random.randint(0,9999),\n",
        "        run2   = random.randint(0,999),\n",
        "        step2  = random.randint(0,9999),\n",
        "    )\n",
        "\n",
        "    atoms_list.append({\n",
        "        \"src_id\":            subdir,\n",
        "        \"bulk_sampling_str\": sampling_str,\n",
        "        \"atoms\":             atoms,      # store the real Atoms w/ calculator\n",
        "    })\n",
        "    print(f\"âœ… Loaded and stored {subdir}\")\n",
        "\n",
        "# write out the pickle\n",
        "with open(output_pickle, \"wb\") as f:\n",
        "    pickle.dump(atoms_list, f)\n",
        "\n",
        "print(f\"\\nğŸ‰ Pickled {len(atoms_list)} structures â†’ {output_pickle}\")\n"
      ],
      "metadata": {
        "id": "cfgot2Fx_Cb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"sec-step7-package\"></a>Â Stepâ€¯7Â â€“Â PackageÂ MergedÂ Pickle\n"
      ],
      "metadata": {
        "id": "uatVlUWtQDrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Paths to your input pickles\n",
        "IN_DOMAIN_PKL       = \"in_domain.pkl\"\n",
        "OUT_OF_DOMAIN_PKL   = \"OutOfDomainBulk.pkl\"\n",
        "MERGED_PKL          = \"Allbulks.pkl\"\n",
        "\n",
        "# 1) Load both lists\n",
        "with open(IN_DOMAIN_PKL, \"rb\") as f:\n",
        "    in_domain = pickle.load(f)\n",
        "\n",
        "with open(OUT_OF_DOMAIN_PKL, \"rb\") as f:\n",
        "    out_domain = pickle.load(f)\n",
        "\n",
        "# 2) Combine and dedupe by src_id\n",
        "combined = in_domain + out_domain\n",
        "unique = {}\n",
        "for entry in combined:\n",
        "    src = entry.get(\"src_id\")\n",
        "    if src not in unique:\n",
        "        unique[src] = entry\n",
        "\n",
        "merged_list = list(unique.values())\n",
        "\n",
        "# 3) Save merged list\n",
        "with open(MERGED_PKL, \"wb\") as f:\n",
        "    pickle.dump(merged_list, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"Loaded {len(in_domain)} inâ€‘domain + {len(out_domain)} outâ€‘ofâ€‘domain entries\")\n",
        "print(f\"Merged (unique) entries: {len(merged_list)} â†’ saved as {MERGED_PKL}\")\n"
      ],
      "metadata": {
        "id": "5k-1HgLiCA54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your pickled data\n",
        "pkl_path = '/content/Allbulks.pkl'\n",
        "\n",
        "# Load the list of dicts\n",
        "with open(pkl_path, 'rb') as f:\n",
        "    bulk_data = pickle.load(f)\n",
        "\n",
        "# Iterate and extract energy\n",
        "for entry in bulk_data:\n",
        "    src_id = entry.get('src_id')\n",
        "    atoms  = entry.get('atoms')\n",
        "    calc   = getattr(atoms, 'calc', None)\n",
        "\n",
        "    if isinstance(calc, SinglePointCalculator):\n",
        "        energy = calc.results.get('energy', None)\n",
        "    else:\n",
        "        energy = None\n",
        "\n",
        "    print(f\"src_id: {src_id}, Total Energy: {energy}\")\n"
      ],
      "metadata": {
        "id": "RJ2mPDXMSXhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out_updated"
      ],
      "metadata": {
        "id": "LYO0CHrbO_Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1ï¸âƒ£  remove the atoms column\n",
        "df_out_clean = df_out_updated.drop(columns=['atoms'])\n",
        "\n",
        "# 2ï¸âƒ£  concatenate the two dataframes row-wise\n",
        "df_merged = pd.concat([df_in, df_out_clean], ignore_index=True)\n",
        "\n",
        "# 3ï¸âƒ£  save to disk without the pandas index\n",
        "csv_name = \"bulk_in_out_merged.csv\"\n",
        "df_merged.to_csv(csv_name, index=False)\n",
        "\n",
        "print(f\"âœ“ Saved merged file â†’ {csv_name}  (rows: {len(df_merged)})\")\n"
      ],
      "metadata": {
        "id": "qs-Wc9IaPD4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"sec-conclusions\"></a>â€¯Conclusions  \n",
        "\n",
        "This notebook demonstrates an **endâ€‘toâ€‘end, reproducible workflow** for discovering and analysing bimetallic bulk materials:\n",
        "\n",
        "1. **Data integration**  \n",
        "   *Queried* theÂ Materialsâ€¯Project for every unique binary combination of fourteen transitionâ€‘ and postâ€‘transitionâ€‘metal elements, then *crossâ€‘referenced* those entries with the **Open CatalystÂ 2020** (OC20) bulks dataset.  \n",
        "   * Materials present in OC20 were labelled **inâ€‘domain**; all others were classified **outâ€‘ofâ€‘domain**.\n",
        "\n",
        "2. **Data enrichment & curation**  \n",
        "   * Filled missing crystalâ€‘symmetry information via local `pymatgen` analysis when absent from the MP record.  \n",
        "   * Completed gaps in key thermodynamic/electronic properties by selecting the most stable polymorph (lowestÂ _E<sub>hull</sub>_) with identical composition and symmetry.\n",
        "\n",
        "3. **Exploratory analysis**  \n",
        "   * Violinâ€‘plot comparison of formation energies revealed systematic stability trends across crystal systems and highlighted differences between *experimentally observed* and *theoretical* compounds.\n",
        "\n",
        "4. **Downâ€‘stream simulation readiness**  \n",
        "   * Automatically generated **VASP** input packages (POSCAR, INCAR, KPOINTS, POTCAR, batch script) for every outâ€‘ofâ€‘domain structure, enabling immediate highâ€‘throughput firstâ€‘principles calculations.\n",
        "\n",
        "---\n",
        "\n",
        "Together, these steps provide a practical template for **highâ€‘throughput screening in catalysis and materials discovery**.  \n",
        "By combining large, curated databases with automated analysis and workflow tools, researchers can quickly target the most promising candidates for further experimental or computational investigation.\n",
        "\n"
      ],
      "metadata": {
        "id": "k1UBtfwlQIoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <a name=\"sec-references\"></a>9.â€¯References  \n",
        "\n",
        "##### 9.1Â DataÂ &Â Code  \n",
        "- **OC20 bulk structures** (`bulks.pkl`) â€“Â ocdata/databases/pkls/bulks.pkl, Open CatalystÂ Dataset. GitHub.  \n",
        "  https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/databases/pkls/bulks.pkl  \n",
        "\n",
        "- **VASP utility functions** (`vasp.py`) â€“Â ocdata/utils/vasp.py, Open CatalystÂ Dataset. GitHub.  \n",
        "  https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset/blob/main/ocdata/utils/vasp.py  \n",
        "\n",
        "##### 9.2Â KeyÂ Publications  \n",
        "-â€¯Chanussot,â€¯L.;â€¯Das,â€¯A.;â€¯Goyal,â€¯S.;â€¯Lavril,â€¯T.;â€¯Shuaibi,â€¯M.;â€¯RiviÃ¨re,â€¯M.;â€¯Tran,â€¯K.;â€¯Herasâ€‘Domingo,â€¯J.;â€¯Ho,â€¯C.;â€¯Hu,â€¯W.;â€¯Palizhati,â€¯A.;â€¯Sriram,â€¯A.;â€¯Wood,â€¯B.;â€¯Yoon,â€¯J.;â€¯Parikh,â€¯D.;â€¯Zitnick,â€¯C.â€¯L.;â€¯Ulissi,â€¯Z.â€¯Open Catalystâ€¯2020â€¯(OC20)â€¯Dataset and Community Challenges. *ACSâ€¯Catalysis* **11**,â€¯6059â€“6080 (2021). DOI:â€¯10.1021/acscatal.0c04525  \n",
        "\n",
        "-â€¯Jain,â€¯A.;â€¯Ong,â€¯S.â€¯P.;â€¯Hautier,â€¯G.;â€¯Chen,â€¯W.;â€¯Richards,â€¯W.â€¯D.;â€¯Dacek,â€¯S.;â€¯Cholia,â€¯S.;â€¯Gunter,â€¯D.;â€¯Skinner,â€¯D.;â€¯Ceder,â€¯G.;â€¯Persson,â€¯K.â€¯A. Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. *APLâ€¯Materials* **1**,â€¯011002 (2013). DOI:â€¯10.1063/1.4812323  \n",
        "\n"
      ],
      "metadata": {
        "id": "Gi1g98EaS36j"
      }
    }
  ]
}